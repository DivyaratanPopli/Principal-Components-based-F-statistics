\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[super]{nth}

\usepackage{lineno}
\usepackage[
singlelinecheck=false
]{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{csvsimple}
\usepackage[section]{placeins}
\usepackage{lineno}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage[breaklinks]{hyperref}
\usepackage{microtype}

\title{Estimating F-statistics in a probabilistic PCA space}
\author{Divyaratan Popli, Benjamin M. Peter}
%\date{5 August 2021}
\linenumbers

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\newcommand{\BZ}{\mathbf{Z}}
\newcommand{\BD}{\mathbf{D}}
\newcommand{\BN}{\mathbf{N}}
\newcommand{\BH}{\mathbf{H}}
\newcommand{\Btheta}{\pmb{\theta}}


\hbadness=99999
\begin{document}

\maketitle


\begin{abstract}

\noindent Principal component analysis (PCA) and $F$-statistics are routinely used in population genetic and archaeogenetic studies. However, these are closely related analyses and reveal the same biological signal. Here, we present a statistical framework to combine them into a joint analysis. In particular, we discuss the differences of probabilistic PCA and ordinary PCA, and show that $F$-statistics are more naturally interpreted in a probabilistic PCA framework. We also show that individual-based $F$-statistics can be accurately estimated from probabilistic PCA in the presence of large amounts of missing data. We compare estimates from probabilistic PCA-based framework to ADMIXTOOLS 2 using simulations and published data, and show that this joint estimation framework addresses limitations of estimating F-statistics and PCA independently.

\end{abstract}

\section{Introduction}

Admixture between previously isolated populations is common in nature, and affects the patterns of genetic diversity. From these patterns of genetic diversity, one can reconstruct the past events of admixture.  

\subsection{Overview of methods to study admixture}
There are several methods available to make inferences about admixture events that explain the observed genetic diversity. These methods can be classified as local or global ancestry based. Local ancestry based methods infer ancestry at each locus and reveal recent history of each individual, but have low power to infer events in deep past \cite{vi_genome-wide_2023, brisbin_pcadmix_2012, price_sensitive_2009, sankararaman_estimating_2008}. Commonly used methods to infer global ancestry are Principal Component Analysis (PCA) \cite{mcvean_genealogical_2009}, MDS \cite{wang_comparison_2009}, STRUCTURE \cite{pritchard_inference_2000} and ADMIXTURE \cite{alexander_fast_2009}. These are powerful methods to infer admixture, but can be difficult to interpret since they do not provide model comparisons or formal tests of admixture.  

\subsection{F-statistics}
F-statistics are a popular way to infer global ancestry. They provide an intuitive and powerful way to test hypothesis of admixture by measuring the genetic drift shared between two, three, or four populations \cite{patterson_ancient_2012, peter_admixture_2016}. In this framework, the null model is represented by a tree connecting the populations, with the branch lengths representing genetic drift. The null hypothesis can be rejected when the variation in observed data can not be explained by a tree-like history. 

It is worth noting that estimating $F$-statistics from data is not trivial. For large sample-size this is a minor issue, but for small sample-size, the necessary bias correction term can be substantial\cite{patterson_ancient_2012}. We provide a detailed definition of F-statistics, and the correction terms for sampling bias in section YY.

\subsection{Estimation of F-statistics}
The issue with estimation of $F$-statistics is further compounded because many species with distributions over large ranges, are not easily subdivided into discrete populations. For humans, it has been a long-standing debate on whether populations as such exist, and to what degree they are the result of biased sampling designs \cite{serre_evidence_2004, rosenberg_clines_2005, peter_genetic_2020}. 

In the context of $F$-statistics, this causes a trade-off between the urge to group as many individuals together as possible (which improves statistical accuracy of statistics, and can help with missingness), but can lead to an overly simplistic view of human genetic structure. On the flip-side, treating each individual independently, will lead to a more accurate description of the overall, individual-level genetic structure, but will be much harder to interpret and the accuracy of statistics will be lower.


These issues are further compounded if the data is low-coverage and of heterogeneous quality, as is common for ancient DNA data sets, where the amount of DNA preserved is often a limiting factor that precludes the generation of high-quality/high-coverage data. In this case, missingness can make individual-based statistics even harder to estimate.

For example, a conservative approach to estimate individual-based F-statistics is to only retain sites where data is present from every single individual in the data set. However, even for moderately large data sets that quickly becomes prohibitive: As a toy example, consider a data set with 100 (haploid) individuals with 10\% missing data, and 1,000,000 sites. Out of those, only 26 would be covered in every single individual and could be used for multivariate analyses. 

However, grouping individuals into populations can dramatically remove the number of sites retained. A common practice is to retain sites if at least one individual in each population has data. In the above example, if we grouped the 100 individuals into 10 populations of 10 individuals each, and retained sites where at least one individual in each population carried data, we would expect no missing sites in almost all cases. However, grouping individuals may not be justified when they do not form discrete clusters, or when there are very few samples whose population assignments are unknown.

These issues can be resolved by estimating individual-based F-statistics with a framework that does not require a priori assignment of individuals to discrete populations, and is not sensitive to randomly missing data.


\subsection{PCA}

PCA is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation while retaining as much of the original variance as possible. It achieves this by finding orthogonal axes, called principal components (PC's), that capture the maximum variance in the data. It is commonly used to understand structure and admixture between populations \cite{patterson_population_2006,novembre_genes_2008,noauthor_cavalli-sforza_nodate,mcvean_genealogical_2009,brisbin_pcadmix_2012}. 

PCA was introduced and popularized as a tool to study human genetic structure by cavalli-sforza, and using just a handful of genetic loci, he was able to use PCA to accurately describe patterns of human genetic variation, and to make inference about their possible causes, although the way these patterns were analyzed were typically quantitative \cite{menozzi_synthetic_1978, sforza_great_1995, noauthor_cavalli-sforza_nodate}.

As is still the norm with $F$-statistics, Cavalli-Sforza aggregated individuals into populations before performing PCA. With the advent of genomic data, his methods became superseded by individual-based approaches, which addressed many of the issues about grouping individuals we discussed above \cite{patterson_population_2006, novembre_genes_2008, price_principal_2006}.


\subsection{Probabilistic PCA and Latent Subspace Estimation (LSE)}

Probabilistic PCA (PPCA) is an extension of PCA that incorporates a probabilistic framework \cite{tipping_probabilistic_nodate}. PPCA models the observed data as generated by a linear transformation of a lower-dimensional latent space $W$ with added Gaussian noise $\Psi$. A similar approach to model the observed data is Latent Subspace Estimation (LSE) \cite{cabreros_likelihood-free_2019}. Here, the noise is modelled as binomially distributed, and so is independent for each individual \cite{van_waaij_evaluation_2023, cabreros_likelihood-free_2019, chen_consistent_2015}. We give a detailed explanation of both the frameworks in section YY. In practice, PCA, PPCA and LSE differ in the way that they model the noise in the observed data due to sampling (see Fig.\ref{fig1:pca_ppca}). 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/pca_all_genetic.png}
    \centering
    \caption{Comparison of PCA, PPCA and LSE: We simulated 100 individuals, and compared the top eigenvalues obtained from different methods.}
    \label{fig1:pca_ppca}
\end{figure}

\subsection{PCA and $F$-statistics}

A common analysis paradigm in ancient DNA is to use PCA for exploratory and descriptive analyses, and then follow them up with methods based on $F$-statistic for a more formal treatment (typically in a third step, methods that synthesize many $F$-statistics are also applied, although we will not cover those here). Because they occur at different stages of the analyses, PCA and F-statistics use different data groupings and different normalizations, and are usually not directly compared. 

Recently, we showed that the information contained in $F$-statistics and PCA is closely related, and that $F$-statistics can be interpreted geometrically in the context of PCA \cite{peter_geometric_2022}. For example, Fig. \ref{fig2:overview} illustrates that when allele frequencies are known,  $F_2(X_1,X_2)$ can be thought of as the squared Eucledian distance between populations $X_1$ and $X_2$ on a PCA. Similarly, $F_3(X_1;X_3,X_4)$ is represented as the length of projection of the vector $X_1-X_3$ on $X_1-X_4$. Internal branch length $F_4(X_1,X_3;X_2,X_4)$ can be described as the length of projection of $X_1-X_3$ on $X_2-X_4$ on PCA, and the test of admixture $F_4(X_1,X_2,X_3,X_4)$ is equivalent to the length of projection of $X_1-X_2$ on $X_3-X_4$. We describe a formal relation between PCA and F-statistics (refer to \cite{peter_geometric_2022} for a detailed derivation) in section YY, but here we point out that a joint framework to estimate PCA and F-statistics not only addresses the issue of population assignment in F-statistics, but also can potentially resolve some issues with the interpretations of PCA \cite{novembre_genes_2008, noauthor_cavalli-sforza_nodate, degiorgio_geographic_2013, francois_principal_2010}. 

However, while \cite{peter_geometric_2022} develops a theoretical link between PCA and $F$-statistics, it does not develop a statistical framework to jointly estimate the two statistics from the data. 

Here, we develop such a framework, and describe the impact of some of the choices of PCA-algorithm (classical PCA, PPC, LSE). In simulations, we show that using PPCA-based $F$-statistics can result in higher accuracy than using the naive estimators, especially when there is missing data. We also draw comparison between PPCA-based framework and ADMIXTOOLS 2 using published Neanderthal samples. At the end, we discuss that this approach not only solves issues related to F-statistics, but also is a step towards standardization and quantification of PCA methods.

\section{Theory}

\subsection{F-statistics}
We follow the original notation \cite{patterson_ancient_2012}, and write the statistical estimates from empirical data as $f_2$, $f_3$ and $f_4$, and denote the theoretical values that depend only on the phylogenetic tree and the ascertainment scheme as $F_2$, $F_3$ and $F_4$. The three F-statistics are defined in terms of population allele frequencies as follows:

\begin{align}\label{eq:f_intro}
F_2(X_1,X_2) &= \frac{1}{S}\sum_{s=1}^S(x_{1s} - x_{2s})^2\nonumber\\
F_3(X_1;X_2,X_3) &= \frac{1}{S}\sum_{s=1}^S(x_{1s} - x_{2s})(x_{1s} - x_{3s})\nonumber\\
F_4(X_1,X_3;X_2,X_4) &= \frac{1}{S}\sum_{s=1}^S(x_{1s} - x_{2s})(x_{3s} - x_{4s}).\nonumber\\
\end{align}

Here, $S$ is the total number of SNPs, and $x_{is}$ is the (unobserved) population allele frequency in population $X_i$ at SNP $s$. Equivalently, we can also assign each individual to a separate population, and so these equations also hold for individuals.

Assuming a tree-like relationship between populations, $F_2(X_1,X_2)$ is interpreted as the branch length between populations $X_1$ and $X_2$ (fig S2 a) and it reflects the expected amount of drift that occurred between $X_1$ and $X_2$. $F_3(X_1;X_2,X_3)$ represents the amount of drift that occurred on the external branch connecting $X_1$ to the common ancestor node of $X_2$ and $X_3$ (fig S2 b). Under a tree-like model, $F_3$ will always be non-negative. However, in the case where $X_1$ is admixed between $X_2$ and $X_3$, $F_3(X_1;X_2,X_3)$ may be negative, and hence this is used as a test for admixture\cite{peter_admixture_2016, patterson_ancient_2012}. 

$F_4(X_1,X_3;X_2,X_4)$ represents the covariance between shared drifts between X1,X2 and X3,X4. This would be represented by the internal branch between the common ancestor nodes of $X_1$, $X_2$ and $X_3$, $X_4$ (fig S2 c). $F_4$ statistic, with a different permutation of the populations can be used as test of admixture. $F_4(X_1,X_2;X_3,X_4)$ is expected to be 0 if $X_1$, $X_2$, $X_3$, $X_4$ are related to each other by a tree (fig S2 d). However, a large positive or negative value would suggest a departure from the null model of treeness due to gene flow between X1 and X3 or X2 and X3 respectively.

Since $F_2$ represents the branch length between a pair of populations, we can write $F_3$ and $F_4$ as linear combination of $F_2$'s.

\begin{align}\label{eq:f3_f4}
F_3(X_1;X_2,X_3) &= \frac{1}{2} [F_2(X_1,X_2) + F_2(X_1,X_3) - F_2(X_2,X_3)]\nonumber\\
F_4(X_1,X_2;X_3,X_4) &= \frac{1}{2} [F_2(X_1,X_3) + F_2(X_2,X_4) - F_2(X_1,X_4) - F_2(X_2,X_3)]\nonumber\\
\end{align}

Patterson et al. showed that the naive estimation of $F_2$ from \emph{sample} allele frequency data will be biased, particularly when the sample size is small. They thus  introduced the bias-corrected estimator \cite{peter_admixture_2016, patterson_ancient_2012}.

\begin{align}\label{eq:f2_correction}
F_2(X_1,X_2) &= \sum_{s=1}^S[(x_{1s} - x_{2s})^2 - \frac{x_{1s}(1-x_{1s})}{n_{1s}-1} - \frac{x_{2s}(1-x_{1s})}{n_{2s}-1}]
\end{align}


This sampling bias affects the estimates of $F_3$ as well but the correction terms cancel out for $F_4$.

\begin{align}\label{eq:f3_correction}
F_3(X_1;X_2,X_3) &= \sum_{s=1}^S(x_{1s} - x_{2s})(x_{1s} - x_{3s}) - \frac{x_{1s}(1-x_{1s})}{n_{1s}-1}
\end{align}

Assuming that the population allele frequencies are known, we can think of F-statistics in terms of Euclidean distances in an allele frequency space \cite{oteo-garcia_geometrical_2021}. In this framework, each population can be represented as a vector in a multi dimensional allele frequency space, and $F_2(X_1, X_2)$ can be calculated as squared Euclidean distance between the vectors (fig. S2 e). $F_3(X_1;X_2,X_3)$ is then a dot product of the vectors $\Vec{x1} - \Vec{x2}$ and $\Vec{x1} - \Vec{x3}$ (fig. S2f). $F_4(X_1,X_3;X_2,X_4)$ is a dot product of vectors $\Vec{x1} - \Vec{x3}$ and $\Vec{x2} - \Vec{x4}$ (fig. S2 f), and $F_4(X_1,X_2;X_3,X_4)$ is a dot product of vectors $\Vec{x1} - \Vec{x2}$ and $\Vec{x3} - \Vec{x4}$ (fig. S2 g). 

\begin{align}\label{eq:f_geometric}
F_2(X_1,X_2) &= \frac{1}{S}||\Vec{x_{1}} - \vec{x_{2}}||^2\nonumber\\
F_3(X_1;X_2,X_3) &= \frac{1}{S}||\vec{x_{1}} - \vec{x_{2}})\cdot(\vec{x_{1}} - \vec{x_{3}})||\nonumber\\
F_4(X_1,X_3;X_2,X_4) &= \frac{1}{S}||(x_{1} - x_{2})\cdot(x_{3} - x_{4})||\nonumber\\
\end{align}

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{ppt/overview.png}
    \centering
    \caption{Schematics showing different interpretations of F-statistics. The columns represent $F_2(X_1,X_4)$, $F_3(X_1;X_3,X_4)$, $F4(X_1,X_3;X_2,X_4)$, $F4(X_1,X_2,X_3,X_4)$. The first row shows a tree interpretation of each statistic, the second row shows F-statistics in an allele-frequency space with three axes representing three SNPs, and the last row is the interpretation of F-statistics on a PCA. Blue lines represent the statistic, and the dotted lines represent orthogonal projections.}
    \label{fig2:overview}
\end{figure}


\subsection{PCA and F-statistics}

This geometric framework provides an alternate way to understand the properties of F-statistics \cite{oteo-garcia_geometrical_2021}. However, many population genetic studies use a large number of SNPs (in the order of a million), and it is not possible to visualize population vectors in such a high dimensional space. Peter at al. showed that one can do dimentionality reduction on such datasets with PCA, and use the top PC's to estimate F-statistics \cite{peter_geometric_2022}.

Let us assume M populations and S SNPs, such that our dataset X has the dimension $[M \times S]$, and each entry of X is an allele frequency $\in$ [0,1]. PCA allows us to project this $[M \times S]$ high dimensional data on a lower dimensional subspace $[q \times M]$. $q = M-1$ represents the case where we retain all the PC's, and thus PCA only rotates X. However, in practice we often only need few PC's (q << M) to explain most variation in the populations \cite{peter_geometric_2022}. A common algorithm to estimate PC's is Singular Value Decomposition (SVD). For this approach we first mean-center X, and then decompose X into U, $\Sigma$, and $V^T$.

$$Y = CX = (U\Sigma ) V^T = WL,$$

where C is the centering matrix such that $C = I - (1/M)I$. We perform SVD to decompose Y into a product of $W = U\Sigma$ and $L = V^T$. In the context of PCA, $W_{MxM}$ is a matrix of principal components and contains information about structure, while $L_{MxS}$, also known as SNP loadings contains the contribution of each SNP to each PC. L can be used to identify outlier SNPs that may be potential candidates for selection \cite{gower_distance_1966}. 

In section YY we showed that F-statistics can be written as the dot product of vectors in an allele frequency space. Dot product is invariant to PCA, which is just a rotation of the dataset to axes of largest variation. Therefore, we can calculate F-statistics from the PC's directly \cite{peter_geometric_2022}:

\begin{align}\label{eq:f_pca}
F_2(X_1,X_2) &= \sum_{s=1}^S(x_{1s} - x_{2s})^2\nonumber\\
&= \sum_{s=1}^S(x_{1s} - \mu_s)(x_{jl} - \mu_s) = F_2(Y_1,Y_2)\nonumber\\
&= \sum_{s=1}^q(w_{1s} - w_{2s})^2 = F_2(W_1,W_2)\nonumber\\
\end{align}

$F_3$ and $F_4$ can then be written in terms of $F_2$, and remain invariable to change in axes. For many applications, we only need few PC's with highest variation to approximate F-statistics \cite{peter_geometric_2022}. 

\subsection{PPCA}
A difficulty in the practical application of this result is that the geometric considerations of \cite{oteo-garcia_geometrical_2021} and \cite{peter_geometric_2022} only hold for the (generally unobserved) population allele frequencies, but not for sample allele frequencies. In ancient DNA, PCA is most commonly run directly on individual-level genotype data \cite{patterson_population_2006}, and hence on the biased sample allele frequencies. Thus, applying the PCA-based estimator (eq. 6) to calculate $F$-statistics would likewise result in biased estimate.


Oteo-García et al. resolved this by only calculating $f_4$ statistics, where the naive estimator is unbiased \cite{oteo-garcia_geometrical_2021}. In \cite{peter_geometric_2022}, unbiased estimates of the PCA reconstructions were obtained indirectly by first calculating all pairwise $F_2$-statistics, and then performing a multidimensional-scaling decomposition equivalent to PCA. 

Here, we develop two related approaches that aim to calculate the bias-corrected estimates of $F_2$ from a PCA, by explicitly separating out the error in allele frequencies.

The first approach is based on probabilistic PCA (PPCA) \cite{tipping_probabilistic_nodate, agrawal_scalable_2020}.
PPCA is a dimensionality reduction technique that extends the classical PCA by introducing a probabilistic framework that allows for a homoskedastic error. Like PCA, it is a statistical model that assumes that the observed data is generated from a lower-dimensional latent space, but it adds a constant error term for each observation.

In classical PCA, the goal is to find a linear transformation (rotation) of the data that captures the maximum amount of variance in the original dataset. However, PCA does not provide a probabilistic interpretation of the data and does not explicitly model noise or uncertainty.

PPCA introduces a probabilistic generative model. It assumes that the observed data points are generated by a linear transformation of a lower-dimensional latent space, with an additional Gaussian noise term. The latent variables capture the underlying structure or patterns in the data, while the noise accounts for variability due to sampling error. Setting the Gaussian noise parameter to 0, converges PPCA to PCA.

PPCA models a latent structure of the form $X \sim N(WZ + \Psi I)$, where X is the observed data, W is a M x q matrix of linear mappings, $\Psi$ is a Gaussian noise term, $Z$ is a $S$-dimensional latent variable, and I is the identity matrix. Intuitively, WZ captures the covariance in the observed data, analogous to the F-statistics, and $\Psi I$ is analogous to the bias-correction term.

The goal of PPCA is to estimate the parameters of the model, namely W, $\mu$, and $\Psi$, given the observed data. This is typically done through the standard expectation-maximization (EM) algorithm or maximum likelihood estimation (MLE).

\subsection{LSE}

LSE is a dimensionality reduction technique quite similar to PPCA, with the difference that LSE accounts for the heteroscedasticity in the data \cite{chen_consistent_2015}, and explicitly models the binomial error in genetic data. In this algorithm, we calculate the heterozygosity  $d_{jj} = \frac{1}{S}\sum_i 2x_{ij}(1- x_{ij})$ from $X$. We define D as a diagonal matrix with $j^{th}$ entry as $\delta_{jj}$. We then calculate the eigenvalues of G = $\frac{1}{S}X^TX - D$. The eigenvectors of G then span the latent subspace of L , and the smallest M-q eigenvalues converge to 0 for large M \cite{cabreros_likelihood-free_2019}.

Using this matrix, we show in the appendix that 
\begin{align}
    f_2(X_i, X_j) &= \sum_s (x_{is} - x_{js})^2 - d_i - d_j \nonumber\\
    &= G_{ii} + G_{jj} - 2 G_{ij}
\end{align}

It is worth noting that the sampling error terms in equation XX are the same as the binomial error model of LSE, and so the distances based on LSE (using all components) are equivalent to those obtained from $F$-statistics (Fig. YY).

\section{Results}

We evaluate the performance of PCA, PPCA and LSE frameworks and compare to that of admixtools 2 using simulations, and a Neandertal dataset. 

\subsection{Evaluation on simulations}
We simulated 10 populations with 10 individuals each using slendr \cite{petr_slendr_2022}. We use a mutation rate of $10^{-8}$ per base per generation, a recombination rate of $10^{-8}$ per base per generation, and a generation time of 30 years. Fig. 2 shows the split times and migration events. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{inkscape/sim1.png}
    \centering
    \caption{Simulated trees for testing our estimation of F-statistics. Split time for each node in generations and the admixture branches are shown in dashed lines. The red arrow represents a migration event from Pop3 to Pop2.}
    \label{fig2:sim}
\end{figure}

We use eq. \ref{eq:f_pca} to calculate $F_2$ for a pair of populations using the  squared Euclidean distance between them from the principal components (see Fig. S2) based on either  PCA, PPCA and LSE, and compare F-statistics estimated from these methods to the true theoretical value of the statistic obtained from branch lengths in slendr \cite{petr_slendr_2022}. We first do a comparison between $f_2$s estimated from PCA, PPCA and LSE with different number of PC's used (Fig. 1). Here, we use 10 individuals in each population, and we see that all three methods are not sensitive to the number of PC's used, as long as the number of PC's is higher than 7. The estimate of F2 based on PCA is only slightly higher than that for PPCA and LSE, since sample  sizes of 10 diploid individuals is large enough that we can ignore the sampling error correction. We next look at a comparison between the three methods using only one individual in each population (Fig. S2). Here we observe that $F2$ estimates from PPCA and LSE are quite close to the true value at all cases with $PCs \geq 7$. However, estimates from PCA get increasingly higher if we add additional PC's. This is expected from theory (section XX), since PCA does not account for sampling bias we end up with the biased estimator $F_2$, and not the bias-corrected $f_2$.


\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{inkscape/sim1.png}
    \centering
    \caption{Simulated trees for testing our estimation of F-statistics. Split time for each node in generations and the admixture branches are shown in dashed lines. The red arrow represents a migration event from Pop3 to Pop2.}
    \label{fig2:sim}
\end{figure}


In the rest of the analyses, we exclude PCA, and compare PPCA and LSE with 8 and 12 PC's to ADMIXTOOLS 2 \cite{maier_limits_2022}, which is a recent reimplementation of ADMIXTOOLS \cite{patterson_ancient_2012}. We chose 8 and 12 PCs because the number of PCs to use will not be known in most applications. We first compare these methods in an ideal scenario, where each population has 10 individuals, and there is no missing data. We estimate $F_2$, $F_3$ and $F_4$  In this case we find that the three methods perform well, and get F-statistics close to the truth (fig. S3). 

We next address the issues listed in Introduction section XX. The first issue is about estimation of F-statistics when population assignment is difficult, especially when few samples are available. We show that this can be resolved with individual-based F-statistics. In our simulations, we label each individual as a different population, and we sample one ancient individual from each population to calculate F-statistics (fig. S4). We observe that in this case both PPCA and LSE- based frameworks perform atleast as well as ADMIXTOOLS 2. The mean estimate from each method is close to the true value, however, the error bars for $F_2$ estimates are lower for PPCA compared to ADMIXTOOLS 2, specially for X1 and X2, which have low split-times. The improved accuracy of PCA-based tools versus ADMIXTOOLS 2 is explained because the PCA incorporates a succinct summary of the full data of all the individuals, and thus the PCA-based estimates can ``borrow'' information from related individuals in the sample that are not used to calculate the statistic at hand. In contrast, admixtools 2 has only one individual from each population to assess structure / admixture, and while the estimates based on admixtools 2 are minimum-variance estimators for these subsets of the data \citep{patterson_ancient_2012}, PCA does better whenever we have data from additional individuals.     

\subsection{Missing data}
Next, we address the issue of missing data and evaluate the estimation of these methods when there is random missing data. Our implementation of PPCA on missing data is inspired from EMU \cite{meisner_large-scale_2021}, and is described in Methods section YY. We see that PPCA is not affected by missingness, while ADMIXTOOLS 2 and EMU results are inflated (Fig. 3). Implementation of LSE is not trivial when there is missingness in data, and is not included in this analysis. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/simfiles/Ne1000/split_times1000/npop10_nind100/missing0.5/plots_8_12/mu0.05_plot_all_1ind_missing.png}
    \centering
    \caption{Comparison of PPCA and PCA to ADMIXTOOLS 2 in presence of 50\% random missingness, using population genotypes from one individual from each population.}
    \label{figS2:pc_scale}
\end{figure}


\subsection{Test of admixture}
A major application of $F$-statistics are tests of admixture (cite). We showed in the previous section that PPCA framework can be used to calculate the point estimates of F-statistics. In this section we show that we can also get standard errors for these estimates using block-jackknife (cite), and use these to do hypothesis testing for admixture. We simulate a gene flow from X3 to X2 500 generations ago with the migration rate of $\mu \epsilon [0, 0.01, 0.05]$. We then compare PPCA framework to ADMIXTOOLS 2. We first test for admixture by checking if the estimate of $F_4(X1,X2,X3,X4)$ is significantly different from 0. We show that when there are 10 individuals in each population, both methods perform well (Fig. S6, table 1). Reducing the number of individuals to 1 from each population reduces the power for both the methods, and with $50\%$ missingness, ADMIXTOOLS 2 loses power to detect admixture, while PPCA-based-framework is not affected as much.

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/simfiles/AvgFolder/Ne1000/split_times1000/npop10_nind100/missing0.5/plots_8/hypothesis_test_comparison.png}
    \centering
    \caption{Test for admixture with individual-based F4 statistic. We compare ADMIXTOOLS 2 (orange)to PPCA-based-framework (blue) in the presence of 50\% missingness in data.}
    \label{figS2:pc_scale}
\end{figure}



\subsection{Evaluation on neandertal dataset}

To test our framework on real data, we apply it to a published dataset of archaic humans from Eurasia \cite{hajdinjak}. This dataset consists of low-coverage late neandertal specimens from Goyet (Goyet\_L35MQ25), Spy (Spy\_L35MQ25), Les Cottés (Les\_Cottes\_L35MQ25), Vindija (VindijaG1\_L35MQ25), and Mezmaiskaya caves (Mezmaiskaya1\_L35MQ25 and Mezmaiskaya2\_L35MQ25) along with the high-coverage archaic specimens from Densiova (Altai, Denisova) and Vindija caves (Vindija33.19). We first estimate PC's for this dataset using PPCA, and show that our plot captures all the features of PCA from the authors (fig. S10, S11). However, we demonstrate that with PPCA, the user can utilize all the specimens to estimate the PC's. 

We analyze how close or distant the low-coverage late Neandertals are to high-coverage Vindija Neandertal using outgroup F3 statistic. F3(Altai, Vindija33.19, X) represents the branch length extending from Altai to the common ancestor of Vindija33.19 and X, where X is a low-coverage late Neandertal. Higher value of F3 denotes closeness of X to Vindija33.19 (fig. S10). We compare the estimates from PPCA-based framework and ADMIXTOOLS 2, and show that they have a very similar pattern (fig. 4). It is interesting to find that $F_3$(Altai, Vindija33.19, VindijaG1\_L35MQ25) estimated by PPCA framework is higher than that by ADMIXTOOLS 2. $F_3$(Altai, Vindija33.19, VindijaG1\_L35MQ25) can be written as [ $F_2$(Altai, Vindija33.19) + $F_2$(Altai, VindijaG1\_L35MQ25) - $F_2$(Vindija33.19, VindijaG1\_L35MQ25) ] /2 .

We looked at the values of the three $F_2$-terms from ADMIXTOOLS 2 and PPCA framework to see why the two methods have different values. We found that $f_2$(Altai, Vindija33.19) and $f_2$(Altai, VindijaG1\_L35MQ25) have values 0.072 and 0.135 from ADMIXTOOLS 2 respectively. Since both Vindija samples are from the same individual, the $f_3$ values should ideally be the same. PPCA framework outputs the values of $f_2$(Altai, Vindija33.19) and $f_2$(Altai, VindijaG1\_L35MQ25) as 0.102 and 0.115, which are in line with the expectation. In addition, $f_2$(Vindija33.19, VindijaG1\_L35MQ25) should ideally be 0. ADMIXTOOLS 2 shows a value of 0.0057, and PPCA-framework gives a value of 0.00094. These results demonstrate that PPCA-based framework performs well with real data, especially when there is high amount of missingness. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/f3_neandertal.png}
    \centering
    \caption{$F_3$(Altai, Vindija33.19, X) estimated with two methods. Larger value on x-axis represents more proximity to Vindija33.19. Bars show 2 standard errors.}
    \label{figS1:pc_scale}
\end{figure}

\section{Methods}

\subsection{PPCA implimentation}

We implement PPCA using maximum-likelihood approach following Tipping and Bishop \cite{tipping_probabilistic_nodate}, and modify this algorithm to work with missing data. Our approach to handle missingness is inspired from EMU \cite{emu}. We describe our algorithm briefly:
\begin{enumerate}
    \item Mean center data $Y = X - \mu$.
    \item Set missing values to 0.
    \item Perform SVD
    \item Calculate the Gaussian noise parameter $\sigma^2 = \frac{1}{M-q} \sum_{k=M-q}^ M v_k^2$ as the sum of square of the $M-q$ smallest eigenvalues.
    \item Obtain the MLE of the eigenvalues as $v_k - \sigma^2$
    \item Calculate the linear mapping matrix $W$ as the top $q$ left eigen vectors multiplied with a diagonal matrix of $q$ corrected eigen values. 
    \item Reconstruct mean-centered data
    \item Replace missing value with reconstructed values
    \item Repeat steps 2-7 until convergence
\end{enumerate}


\subsection{Calculation of standard errors}

We use a block-jackknife approach to calculate standard errors \cite{maier_limits_2022}. We divide the genome in 2 MB (check) blocks, and estimate PC's and then F-statistics removing a block. Since the statistics obtained are not independent, we calculate variance using this equation \cite{maier_limits_2022}:

\begin{align}\label{eq:bjk_var}
V &= \frac{1}{g} \sum_{i=1}^g \frac{s_i}{S-s_i} (\hat{\theta} - \theta_i)^2
\end{align}

Here, V is the variance of a statistic $\theta$, g is the number of blocks, s is the number of sites in block i, and S is the total number of sites.


\section{Discussion}
-fstatistics estimation from ppca solves all problems
- lse similar to ppca, 
- pca contains noise
- fstats provides a way to quantify pca, so answers eran's question of which pc to show
- pca is normalized differently for different softwares. should be aligned to f-statistics 


We show that F-statistics estimated from LSE using all eigenvalues is the same as F-statistics calculated with ADMIXTOOLS 2 (add this supplemenatry fig), and when we use the top PC's, LSE and PPCA show similar results. We show that individual-based F2's get inflated when estimated with PCA, but not with PPCA or LSE framework. We provide a PPCA-based framework to estimate F-statistics even in the presence of random missing data. We compare F-statistics estimated from our framework to ADMIXTOOLS 2, and find that in the ideal case, when there are many individuals in each population, and there is no missing data, all the methods perform well. However, while working with ancient DNA, we often encounter populations with few individuals, who may not belong to discrete populations. We find that PPCA-based framework performs better on simulations to estimate individual-base F-statistics in presence of large amounts of missingness. We show that a PPCA-based framework is a step towards estimating individual-based F-statistics, without assigning the individuals to discrete populations.  

PCA is widely used in population genetics to visualize clusters of individuals that may represent populations, and clines potentially representing historical admixture. In addition PCA may reveal fine-scale structure in the population \cite{Ashkenazi Jewish}. PCA's ability to condense complex genetic data into interpretable dimensions enhances our understanding of human evolution, migration, and admixture events, and throws light on the intricate mosaic of our species' history. However the visualization results from PCA may be influenced by the choice of PC's used, normalization, and the choice of populations used. \cite{elhaik_principal_2022}. We provide a way of quantifying the results of a PCA with F-statistics  so that all PCA's can be comparable and can use the same normalizations. Such a quantification using all the top PC's also makes it more straight-forward to justify a visual result made with 2 PC's.

We used a PPCA-based framework with the Neandertal data to estimate PC's utilizing all the samples. This approach is more straight-forward than authors' approach to first estimate the PC's using high-coverage genomes, and then project the low-coverage genomes. We show that we can accurately estimate F-statistics using the PC's, even with pseudo-haploid data.

One limitation of this framework is the need to perform multthly and informal “data science lunch” that I’ll
host in my department’s open seminar area on the last Tuesday of each
month.Let’s try to start next week and welcome Fernando, who just joined
Jenny’s department. I will sit in the open seminar area of my
department starting at 12pm and eat my lunch.iple PPCAs to obtain standard errors, which can be computationally expensive. Further studies are needed to design a statistical framework that can estimate the errors using SNP loadings, and therefore can work fast with large datasets.

To summarize, we present a method to perform PCA and F-statistics jointly and show that this approach not only improves estimates of F-statistics, but also provides a solution to the standardization and quantification of PCA. Our framework is available on github as a snakemake pipeline: https://github.com/DivyaratanPopli/A-joint-framework-for-PCA-and-F-statistics.


\section{Supplementary Figures}

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{inkscape/pca.png}
    \centering
    \caption{Calculation of $F_2(Pop1,Pop3)$. Figure shows that $F_2$ can be calculated as the squared Euclidean sum between 2 points using all the relevant PC's (see eq. \ref{eq:f_pca}).}
    \label{figS1:pc_scale}
\end{figure}

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/supplementary/Ne1000_split_times1000_npop10_nind100_mu0_f2_plot_scale_test.png}
    \centering
    \caption{Comparison of PCA, PPCA and LSE using population allele frequencies of 10 individuals in each population. X-axis shows the number of PC's used for the three methods, ranging from 2 to 50.}
    \label{figS1:pc_scale}
\end{figure}


\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/supplementary/Ne1000_split_times1000_npop10_nind100_mu0_f2_plot_scale_test_ind.png}
    \centering
    \caption{Comparison of PCA, PPCA and LSE using population genotypes from one individual in each population. X-axis shows the number of PC's used for the three methods, ranging from 2 to 50.}
    \label{figS2:pc_scale}
\end{figure}


\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/simfiles/Ne1000/split_times1000/npop10_nind100/plots_8_12/mu0.05_plot_all.png}
    \centering
    \caption{Comparison of PPCA and LSE to ADMIXTOOLS 2 using population genotypes from ten individuals in each population.}
    \label{figS2:pc_scale}
\end{figure}

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/simfiles/Ne1000/split_times1000/npop10_nind100/plots_8_12/mu0.05_plot_all_1ind.png}
    \centering
    \caption{Comparison of PPCA and LSE to ADMIXTOOLS 2 using population genotypes from one individual from each population.}
    \label{figS2:pc_scale}
\end{figure}

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/simfiles/AvgFolder/Ne1000/split_times1000/npop10_nind100/missing0/plots_8/hypothesis_test_comparison.png}
    \centering
    \caption{Test for admixture with F4 statistic. We compare ADMIXTOOLS 2 (orange)to PPCA-based-framework (blue).}
    \label{figS2:pc_scale}
\end{figure}


\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/neandertal_pca_smartpca.png}
    \centering
    \caption{PPCA of archaic specimens}
    \label{figS2:pc_scale}
\end{figure}

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/neandertal_ppca.png}
    \centering
    \caption{PCA of archaic neandertals created with smartpca with high coverage neandertals, and projection of the rest.}
    \label{figS2:pc_scale}
\end{figure}


\section{Appendix}

\begin{equation}
    \hat{\mathbf{H}} = \frac{1}{m}\mathbf{X}^T\mathbf{X} - \hat{\mathbf{D}},
\end{equation}

Here, $\MX$ is the (uncentered) genotype matrix and $\hat{\mathbf{D}}$ is a matrix of diagonal entries ``correcting'' heterozygosities and $m$ is the number of SNPs.

In particular $d_{ij} = 0 $ for $i \neq j$ and, 
\begin{equation}
    d_{ii} = d_i = \frac{1}{m}\sum_{k=1}^m x_{ik}(2-x_{ik}),
\end{equation}
for diploid data. The correction term given by Reich 2009 and Patterson 2012 is different by a factor of four:
\begin{equation*}
    d_{ii}' = \frac{1}{m}\sum_{k=1}^m \frac{x_{ik}(2-x_{ik})}{4} = \frac{d_{ii}}{4}
\end{equation*}.
The difference is explained that they use allele frequencies instead of genotypes, i.e. their approach would use (again for diploid data)
\begin{equation*}
    \hat{\mathbf{H}'} = \frac{1}{m}\frac{\mathbf{X}^T}{2}\frac{\mathbf{X}}{2} - \frac{\hat{\mathbf{D'}}}{4} = \frac{\hat{\mathbf{H}}}{4},
\end{equation*}
and so the parametrizations are equivalent, but will differ by a factor of four.

Thus, (TODO: double-check if rows/columns are aligned, sum should be over SNP)

\begin{subequations}\begin{align}
    h_{ii} &= \frac{1}{m}\sum_{k=1}^m x_{ik}^2 - d_i \\
    h_{ij} &= \frac{1}{m}\sum_{k=1}^m x_{ik}x_{jk} 
\end{align}\end{subequations}
Consider now
\begin{align}
    f_{ij} &= h_{ii} + h_{jj} - 2 h_{ij}\nonumber\\
     &= \frac{1}{m}\sum_{k=1}^m x_{ik}^2 - d_i + \frac{1}{m}\sum_{k=1}^m x_{jk}^2 - d_j - \frac{2}{m}\sum_{k=1}^m x_{ik}x_{jk}\nonumber \\
    &= \frac{1}{m} \sum_{k=1}^m (x_{ik} - x_{jk})^2 - d_i - d_j \\
    &= F_2(i,j)\nonumber
\end{align}

Hence the matrix $\hat{\mathbf{H}}$ can be used to estimate $F_2$-statistics (possibly instead of PPCA?).



\section{removed stuff}

We describe two issues in accurate estimation of population allele frequencies:

1. Humans may not fit into well-differentiated discrete populations, except in cases where the populations have been isolated due to geographical features \cite{novembre_genes_2008}. The estimation of population allele frequencies depends on the assignment of individuals to discrete populations, and may be affected by miss-assignment especially when few samples are available. 

2. Missing data in some individuals for certain sites can make it difficult to get accurate allele frequency estimates at those sites. One commonly used solution to this problem is to filter out all the sites with missing data. However, this may make the number of sites available for F-statistics quite small. E.g., for 100 individuals with $10\%$ randomly missing sites, the available number of sites after filtering out positions with missing data would be $\approx26$ out of a total 1,000,000 sites. 

Studies using PCA generally use specific PC's to visualize population structure or admixture, and the choice of the PC's used can be quite subjective \cite{elhaik_principal_2022}. Estimation of F-statistics from PCA quantifies, using all the important PC's, what the researcher has visualized using seemingly arbitrary PC's.

However, a limitation with such a framework is that it is possible to define F-statistics in terms of PC's only when allele frequencies are known, and need not be estimated. This is due to the fact that PCA does not filter the noise in the data due to sampling. In addition, missing data can affect the computation of PC's, and subsequently F-statistics.


We would point out here that in ancient DNA studies PCA is sometimes used as quality-control step by constructing PC's using high-quality samples, and projecting the low-quality samples which may be from the same populations or even the same individuals as the high coverage samples. In the presence of contamination from present-day people, reference bias, ascertainment bias or batch effects, the projected sample may not overlap with an identical high-coverage sample. These biases and issues are not resolved with PPCA/LSE either, since PPCA only models sampling noise. 


\subsection{F-statistics with PPCA/LSE}
In this study, we develop a statistical framework to estimate F-statistics between individuals in a PPCA framework. We show that PPCA explicitly models the error due to the sampling bias in allele frequencies. In addition, we demonstrate that PPCA based framework is not sensitive to random missing data, and so it can be used to visualize individuals in PCA-space without having to project lower quality samples.

We explain that PPCA provides a natural framework to estimate F-statistics with small sample-size and missing data. We show formal hypothesis tests for admixture and compare our results to admixtools2 \cite{maier_limits_2022} on simulations. Finally we show the use of this framework on published datasets from neolithic Haak et al. and upper Paleolithic humans (Mateja). 

Mathematically, the PPCA model can be represented as follows \cite{tipping_probabilistic_nodate}:

Latent variable model:
Latent variables: $Z \sim N(0, I)$, where Z is a S-dimensional latent variable, and I is the identity matrix.

Latent-to-observed mapping: $X = WZ + \mu + \Psi$, where X is the observed data, W is a M x q matrix of linear mappings, $\mu$ is the mean of the observed data, and $\Psi$ is a Gaussian noise term.

Prior distributions:

Prior on the latent variables: $P(Z) = N(0, I)$

Prior on the noise term: $P(\Psi) = N(0, \sigma^2I)$, where $\sigma^2$ is the variance of the noise.

Likelihood function:
$p(X|Z, W, \mu, \Psi) = N(X|WZ + \mu, \sigma^2I)$

\section{old stuff}
\subsection{PCA}

One way to estimate F-statistics 

the estimation of allele frequencies can also be affected by large amounts of missing data. Since F-statistics are dependent on the ascertainment scheme, random missing data reduces the number of overlapping sites greatly.
PCA is a powerful method to visualize population structure but it can be difficult to interpret \citep{cavalli-sforza1993, novembre_stephens_2008, degiorgio_rosenberg2013}. In particular, the population structure in PCA is a function of expected pairwise coalescence times \citep{mcvean_genealogical_2009}, and thus is not explicitly tied to a particular scenario; different histories may yield similar or identical PCAs. 

Thus, parameter estimation, model comparisions and formal tests of admixture are usually not carried out in PCA. 

(add section here introducing the general idea of PCA, explain how it deals with error/noise  and difference between probabilistic and regular PCA. Possibly also explain how people use PCA as a QC-step to detect batch effects, and how that compares with PCA used for population structure (i.e. Ainash' question from your talk))

F-statistics are a useful tool to quantify population structure, and provide tests for admixture. Hence, a common pipeline in many population genetic studies is to analyze PCA plots to look for visible patterns that could be due to past admixtures, followed by formal test with F-statistics \cite{lazaridis_ancient_2014,lazaridis_genomic_2016}. F-statistics use population allele frequencies to test for admixture, and hence the accuracy of these tests is limited by the accuracy of allele frequency estimates \cite{peter_admixture_2016}. Whereas F-statistics estimates are robust when there are enough high quality samples, we show that low number of samples and missing data decrease the accuracy of these tests.

A major issue when working with ancient DNA is low number of individuals and missing data. This issue is exacerbated due to difficulty in assigning some of the individuals to discrete populations. Especially in the case of humans samples, we know that the genetic samples do not strictly belong to discrete populations, but form a continuous spectrum in the allele frequency space \cite{oteo-garcia_geometrical_2021}. Hence, it is a step in the right direction to think of a method to estimate F-statistics in a structure-aware framework. The easiest way to think about this is using PCA. PCA does not require assignment of individuals to discrete populations, and it has been shown that F-statistics can be estimated conveniently from distance between populations on PCA space \cite{peter_geometric_2022}. However, PCA distances are inaccurate when population sizes are small since PCA does not explicitly model sampling bias in the allele frequencies (see section XX). In addition, PCA is sensitive to missing data, and this makes it difficult to work with ancient DNA. 



In this study, we develop a statistical framework to estimate F-statistics between many populations in a probabilistic PCA framework.  that probabilistic PCA (PPCA) explicitly models the error due to the sampling bias in allele frequencies. In addition. we demonstrate that PPCA based framework is not sensitive to random missing data, and so it can be used to visualize individuals in PCA-space without having to project lower quality samples.

Finally, we show that PPCA provides a natural framework to estimate F-statistics with small sample-size and missing data. We show formal hypothesis tests for admixture and compare our results to admixtools2 cite{admixtools paper} on simulations. Finally we show the use of this framework on published datasets from neolithic Haak et al. and upper Paleolithic humans (Mateja). 





\section{older stuff}
There are several tools available to understand genetic diversity, and can be classified as the tools that make minimal assumptions to summarize population structure, and the tools that infer demographic parameters. Former set of tools includes f-statistics \cite{patterson_ancient_2012}, PCA \cite{mcvean_geneaylab=""logical_2009}, MDS \cite{wang_comparison_2009}, Structure \cite{pritchard}, Admixture  
- There are different tools to study diversity: 1) tools that make minimum assumptions like fstatistics, PCA, MDS, Structure, Admixture, 2) tools that can be used to infer demographic parameters.
- Many studies use PCA followed by f-statistics in their pipline. Peter et al., show that these analyses reveal the same biological signal, and can be done jointly.
- This framework has limitation: population allele frequencies are not known.
- Here we present an approach based on pPCA, and show that it is a more natural framework since it can take in account the errors associated with allele frequency estimation.

\subsection{Theory}
- f-statistics, and the sampling error terms in f2.
- PCA and relation to f-statstics.
- pPCA/PCA1 (Waaij et. al.) as a framework for dimentionality reduction taking in account the sampling error.

\subsection{results}
- Advantage in estimating PCs: Fig.1: PCA plot with standard errors

- Advantage in estimating f-statistics: We compare f-statistics from pPCA, PCA1, PCA, admixtools2 in terms of accuracy (and speed?). Fig.2: point estimates of f2's for slendr simulations where we have both ancient and modern populations. Fig.3: Examples of f4 test of treeness with different cases of migrations, we can also show a case of f3 test of admixture.

- Application to a published dataset (Fig.4)

\subsection{Discussion}
- One key advantage of this framework is that both point estimates and standard errors for PCA and fstatistics are estimated together in a consistent way.
- This is a step towards solving the issue with the assumption of discrete populations.
- This would be quite useful for cases where assigning individuals to populations can be difficult.
- Future work: 1) A faster way of estimating standard errors. 1) to get uncertainty from snp loading. 2) Hypothesis testing 

- This approach could also help with missingness as shown by Meisner et al. 
- pPCA makes it easy to analyse modern and ancient data together without having to project samples.

\subsection{Methods}
- Simulation method and parameters used


\section{Abstract}

Studies of genetic variation now routinely include data from thousands of individuals representing complex historical and temporal structure. Understanding and modeling patterns of genetic variation between large numbers of individuals and populations is thus a key challenge in the usage of genetic data to answer questions about evolutionary history. 
Principle Component Analysis (PCA) and F-statistics sensu Patterson are both widely used for this purpose, but are usually analyzed dis-jointly. Here, we present a new framework based on 
probabilistic PCA to jointly estimate principal component and F-statistics from large panels of data. A key advantage of our approach is that we can calculate individual-based F-statistics efficiently, and so population assignments become a result rather than an a priori assumption. Furthermore, probabilistic PCA provides a natural framework for incorporating missing data, a common issue in ancient DNA analyses. Taken together, our results greatly simplify the analysis of large population genetic data sets, and allow for fast data exploration and statistical testing in a unified and consistent framework.

\section{Background}

\subsection{Why study genetic diversity}
The genetic diversity of human populations has been shaped by historical and environmental factors over hundreds of thousands of years. Therefore, a key objective of population genetics is to analyze the observable variations and patterns in order to understand and reconstruct the demographic and evolutionary history of our species. 

\subsection{The general pipeline}

A general pipeline consists of a method to summarize the data with minimal assumptions. Examples are PCA, MDS, Structure, Admixture. These methods show a qualitative picture, but do not estimate a biologically meaningful parameter. And so, it is difficult to design statistical tests for the results of such methods. However, such qualitative results are generally followed by quantifiable methods based on f-statistics. F-statistics with 2,3, or 4 populations assumes a null-model as a tree-like structure and a deviation from the tree-like structure is represented the alternate model.

\subsection{PCA and fstatistics}
PCA is a method to rotate the dataset in a way that so that the axes that are analysed and plotted are aligned with the dimensions explaining the highest variation in the data. This is a way to do dimensionality reduction, and provides a way to do better data visualization. Ben[2020] showed that PCA and f-statistics are related, and   


\section{Introduction}
\subsection{Why combine pPCA and F-statistics?}

\paragraph{In case of no missing data}

- Calculating f-statistics between populations already assumes clustering of individuals into populations. PCA shows this clustering, but it would be useful to quantify the distance between individuals to filter for individuals that cluster, spot outliers, and identify substructures.

-Faster calculation of f-statistics, when only few PCs are used. Calculation of pPCA may take some time, but afterwards f-statistics is less time taking. And since people anyway do both PCA and f-statistics, overall this would reduce time.

-admixture proportions from f-statistics using pca. We can check if it's more reliable.
\paragraph{In case of missing data}

- In case of admixturegraphs, missing data may reduce total snps (although, missing sites would be less if allele frequencies are calculated from available individuals in populations). pPCA may help in cases with e.g., few ancient individuals from each population with a lot of missing data. 

- In case of ancient DNA, may help to include libraries with missing data instead of projecting them?
\newpage
\section{Practical application ideas}
This is a list of ideas we could pursue. Goal would be to pick a few that would be easiest to accomplish.
\begin{enumerate}
    \item Evaluate whether we use PPCA to calculate individual-based $F$-statistics accurately and fast?
    \begin{enumerate}
        \item in presence of missing data
        \item for multivariate analyses including qpadm/qpgraph
        \item include samples projected onto PCA
    \end{enumerate}
    \item Grouping individuals in populations
    
    \item Can we get confidence intervals on PCA?
        \begin{enumerate}
            \item Resampling SNPs
            \item Resampling individuals
            \item Calculate uncertainty based on SNP loadings
            \begin{enumerate}
                \item For SVD $\mathbf{X} = \mathbf{(UD)V}^T = \mathbf{PV}^T$, we might be able to use the correlation in the entries of $\mathbf{V}$ to estimate the ``effective'' number of SNPs
            \end{enumerate}
        \end{enumerate}
    \item Practical programming
        \begin{enumerate}
            \item Write software tool that jointly computes individidual-based F-stats and PCA
        \end{enumerate}
    \item Data analysis -- find a good data set or scenario to analyze
        \begin{enumerate}
            \item Standard Western Eurasian PCA
            \item Indian data
            \item Some application that Stephan / Wolfgang are working on
        \end{enumerate}
    \item Looking at qpadm/qpwave
        \begin{enumerate}
            \item qpadm projects samples into a subspace made from a subset of samples. As these samples are not orthogonal, this subspace is likely highly non-orthogonal and therefore tricky to work with. Doing PCA before doing qpadm could be helpful. 
        \end{enumerate}        
\end{enumerate}


\section{Projecting onto prob PCA}

For PCA, we can write $X_{[n \times p]} = USV^T$ and the PCs are given by $SV^T$. , and we would project by


\begin{align}
    X &= USV^T \\
    U^T X &= SV^T \\
    XVS^{-1} &= USV^TVS^{-1} \\
    &= U \\
    U^T Y &= Y_{proj}\\
    XVS^{-1}Y &= Y_{proj}
\end{align}
and we can project using $U^TY $ for new data $Y$. 


\newpage
\section{F-tests using Wishart-log-likelihoods}
Consider the $F_4$-statistic
\begin{align}
    F_4(A-B, C-D)&= Cov(A-B, C-D)\\ &= Cov(A, C) + Cov(B,D) - Cov(A,D) - Cov(B,D)\\
\end{align}


\subsection{PCA to Covariance matrix}
If we have a matrix of PCs, $\BP$, then $\mathbf{Y} = \BP\BP^T$ is an estimate of the covariance matrix. Consider the random variables $(A-B)$ and $(C-D)$. If we assume they are jointly normally distributed, then their joint distribution will again be normally distributed with mean zero and covariance matrix 
\begin{equation}
    \bf{X} = \begin{pmatrix} 
    y_{11} + y_{22} - 2 y_{12} & y_{13} + y_{24} - y_{14} - y_{23} \\
    y_{13} + y_{24} - y_{14} - y_{23} & y_{33} + y_{44} - 2 y_{34} &  \\
    \end{pmatrix}
\end{equation}
where the $y$ are the entries of the  covariance matrix obtained from the PCA. Practically, we can either first calculate $Y$ (if we want all $F$-stats), or first subset $\BP$ to the four pops involved.

The off-diagonal elements of $\MX$ are precisely the $F$-statistics we aim to calculate. And thus we set up a test to see whether they are zero.


\subsection{Derivation of the test statistic}
The sampling distribution of a covariance follows a Wishart distribution. This is a $p \times p$ - matrix-valued probability distribution that is parametrized by a degree-of-freedom parameter $n$ and a covariance matrix $\MS$, also of dimension $p \times p$. The simplest way to generate Wishart random variates is, 
\begin{equation}
    \MX = \sum_{i=1}^n Y_i Y_i^T
\end{equation}
where $Y_i \sim N(0, \MS)$. We also have 
\begin{align}
    E[\MX] &= n\MS \\
    mode(\MX) &= (n-p-1) \MS
\end{align}

The log-likelihood of a Wishart Distribution is

\begin{align}
    \log P(\MX | \MS, n) &= -\frac{np}{2} \log(2) -  \frac{n}{2}\log\left|\MS\right| - \Gamma_p\left(\frac {n}{2}\right ) + \frac{n-p-1}{2} \left|\MX\right| -\frac{1}{2}\operatorname{tr}({\MS}^{-1}\MX)\\
    &\propto  -\frac{n}2\log\left|\MS\right|   -\frac{1}{2}\operatorname{tr}({\MS}^{-1}\MX)\nonumber\\
    %&= -\frac{n}2 \log \left( \sigma_{11}\sigma_{22} - \sigma_{12}^2\right) - \frac{1}{2}\frac{\sigma_{22}x_{11} - 2\sigma_{12}x_{12} + \sigma_{11}x_{22}} {\sigma_{11}\sigma_{22} -  \sigma_{12}^2} 
    &= -\frac{n}2 \log \left( \sigma_{11}\sigma_{22} - \sigma_{12}^2\right) - \frac{1}{2}\frac{\sigma_{22}x_{11} + \sigma_{11}x_{22} -2 \sigma_{12}x_{12}} {\sigma_{11}\sigma_{22} -  \sigma_{12}^2} 
\end{align}
where the last step assumes a $2\times 2$ matrix. 
Under H0, we have $\sigma_{12} = 0$, therefore
\begin{align}
    \log P(\MX | \MS_0, n) &= -\frac{n}2 \log \left( \sigma_{11}\sigma_{22}\right) - \frac{1}{2}\left[\frac{x_{11}}{\sigma_{11}}  + \frac{x_{22}}{\sigma_{22}}\right] 
\end{align}
This likelihood is easily separatable and we can estimate $\sigma_{11}$ from $x_{11}$ and $\sigma_{22}$ from $x_{22}$ directly.

The log-likelihood-ratio statistic can then be calculated as 

\begin{align}
    R &= - 2 \log  \left( \frac{P(\MX | \MS_0, n)}{ P(\MX | \MS, n)} \right)  \nonumber\\
    &=2 [\log P(\MX | \MS, n) - \log P(\MX | \MS_0, n) ] \nonumber\\
    &=  n \log \left( \sigma_{11}\sigma_{22}\right) - n \log \left( \sigma_{11}\sigma_{22} - \sigma_{12}^2\right)  \nonumber\\
    &+  n\left[\frac{x_{11}}{\sigma_{11}}  + \frac{x_{22}}{\sigma_{22}}\right] 
    - n\frac{\sigma_{22}x_{11} + \sigma_{11}x_{22} -2 \sigma_{12}x_{12}} {\sigma_{11}\sigma_{22} -  \sigma_{12}^2} 
\end{align}
using the estimates $\sigma_{ij} = \frac{x_{ij}}{n}$ we get
\begin{align}
    R&\approx n \log \left( x_{11}x_{22}\right) - n \log \left( x_{11}x_{22} - x_{12}^2\right) + 2n - 2n  \nonumber\\
     &= n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)
\end{align}
 A $\log(n^2)$ term in each of the log-terms cancels. The statistic $R$ is asymptotically $\chi^2$ distributed with one degree of freedom.

If we instead use the mode $\sigma_{ij} \approx \frac{x_{ij}}{n-3}$:

\begin{align}
    R&\approx n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)
+2 (n-3) \frac{x_{11}x_{22} - x_{12}^2} {x_{11}x_{22} -  x_{12}^2} 
    - 2(n-3) \nonumber\\
     &= n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)
\end{align}

\subsection{Comparison to Cavalli-Sforza \& Piazza, 1975}
The authors propose 
\begin{equation}
    R = \frac{|\MX|}{|\MS|}
\end{equation}
where $|\cdot|$ is the determinant. For $2 \times 2$ matrices,


\begin{align}
|\MX| &= x_{11}x_{22} - x_{12}^2 \\
|\MS| &= x_{11}x_{22} \\
    R &= \frac{ x_{11}x_{22} - x_{12}^2}{ x_{11}x_{22}} =1 - \frac{ x_{12}^2}{ x_{11}x_{22}}\\
    T &= -2 n \log (R) = 2 n \log \left( \frac{ x_{11}x_{22}}{ x_{11}x_{22} - x_{12}^2}\right)
\end{align}

The factor of 2 might be wrong..

\subsection{other statistics}
for $x_{12}$  small, we may further approximate
\begin{align}
         R &= n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)\nonumber\\
            &= n  \log ( x_{11}x_{22}) - n\log ({x_{11}x_{22} - x_{12}^2} )\nonumber\\
            &= n  \log ( x_{11}x_{22}) - n\log \left[x_{11}x_{22} \left(1  - \frac{x_{12}^2}{x_{11}x_{22}} \right)\right]\nonumber\\
           &=n  \log \left( 1 - \frac{x_{12}^2}{x_{11}x_{22}}  \right) \nonumber\\
           &\approx  n \frac{x_{12}^2}{x_{11}x_{22}} 
\end{align}
This is the coefficient of determination, which is the square of the correlation coefficient
\begin{equation}
    r =  \sqrt{R/n} = \frac{x_{12}}{\sqrt{x_{11}x_{22}}}
\end{equation}

for which we have a $t$-distributed null
\begin{equation}
    r \sqrt{\frac{n-2}{1-r^2}} \sim t(n)
\end{equation}
The Fisher-Transform then yields
\begin{equation}
    \frac{1}{2} \log\left(\frac{1+r}{1-r}\right) = \arctan(r) \sim N(0,1)
\end{equation}
which simplifies to 
\begin{equation}
    \arctan(r) = \frac{1}{2} \log\left(\frac{\sqrt{x_{11}x_{22}} + x_{12}}{\sqrt{x_{11}x_{22}} - x_{12}}\right) 
\end{equation}
This statistic is normally distributed under the null-hypothesis.


in tests with a simple bivariate normal, all of them behaved equally well.


\newpage
\section{Calibrating the standard errors of $F$-stats}
Traditionally, the standard errors of F-stats are estimated using a block-Jackknife approach. However, the block size is usually hard to estimate, and may impact the resulting values.

\subsection{What do the standard errors measure?}
There are two types of uncertainty:
\begin{itemize}
    \item \textbf{sampling uncertainty}, that stems from the fact that we only have a small sample from each population. Because it depends on the sample, we expect these uncertainties to be independent for each sampled population
    \item \textbf{evolutionary uncertainty} there is also uncertainty due to the randomness in evolution. In particular, the realized mean allele frequencies in populations will be different from those expected under some model. 
\end{itemize}

\subsection{Covariance of $F_2$-statistics}
\begin{align}
    K &= Cov( (X_1- X_2)^2, (X_3 - X_4)^2 )  \nonumber\\
    &= Cov(X_1^2, X_3^2) + Cov(X_1^2, X_4^2) + Cov(X_2^2, X_3^2) + Cov(X_2^2, X_4^2) \nonumber\\
    &- 2\left [ Cov(X_1^2, X_3X_4) + Cov(X_2^2, X_3X_4) + Cov(X_3^2, X_1X_2) + Cov(X_4^2, X_1X_2)) \right]\nonumber\\
    &+ 4\left[ Cov(X_1X_2, X_3X_4)\right]\nonumber\\
\end{align}
At the same time, we have

\begin{align}
    F_4^2 &=  Cov(X_1 - X_2, X_3 - X_4) ^2 \nonumber\\
    &= \big[ Cov(X_1, X_3) + Cov(X_2, X_4) - Cov(X_1, X_4) - Cov(X_2, X_3)   \big]^2\nonumber\\
    &= Cov(X_1, X_3)^2 + Cov(X_2, X_4)^2 + Cov(X_1, X_4)^2 + Cov(X_2, X_3)^2  \nonumber\\
    &+2 \big[ Cov(X_1, X_3) Cov(X_2, X_4) + Cov(X_1, X_4) Cov(X_2, X_3)\big] \nonumber\\
    &-2 \big[ Cov(X_1, X_3) Cov(X_1, X_4) + Cov(X_1, X_3) Cov(X_2, X_3)\big] \nonumber\\
    &-2 \big[ Cov(X_2, X_4) Cov(X_1, X_4) + Cov(X_2, X_4) Cov(X_2, X_3)\big] \\
    &= (E[X_1^2] + E[X_2^2])(E[X_3^2] + E[X_4^2]) \nonumber\\
    &+2 \big[ E[X_1X_3]E[X_2X_4] + E[X_1 X_4] E[X_2 X_3]\big] \nonumber\\
    &-2 \big[ E[X_1 X_3] E[X_1 X_4] + E[X_1 X_3] E[X_2 X_3]\big] \nonumber\\
    &-2 \big[ EX_2 X_4] E[X_2 X_3] + E[X_2 X_4] E[X_2 X_3]\big] \\
    &= (E[X_1^2] + E[X_2^2])(E[X_3^2] + E[X_4^2]) \nonumber\\
    &+2 \big[ E[X_1X_3]  [ E[X_2X_4] - E[X_1 X_4]] +  E[X_2 X_3] [ E[X_1 X_4]  - E[X_1 X_3] ]\big] \nonumber\\
    &-2 \big[ EX_2 X_4] E[X_2 X_3] + E[X_2 X_4] E[X_2 X_3]\big] \nonumber\\
\end{align}
But we have $Cov(A,B) = E[ (A-E[A])(B-E[B])] = E[AB] - E[A]E[B]$ and 
\begin{align}
Cov(A, B)Cov(C, D) &= E[ (A-E[A]) (B-E[B])] E[(C-E[C])(D-E[D])]\nonumber\\
&= E[AB]E[CD] =  E[ABCD] - Cov(AB, CD)
\end{align}

\newpage
\section{Notes on PCA1 of van Waaij et al}
van Waaij et al. suggest at PCA on a matrix of the following form for PCA
\url{https://doi.org/10.48550/arXiv.2302.04596} (their PCA1). This approach is further motivated by eq 7 in \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6707457/}, which is a very technical (but probably useful) reference.

\begin{equation}
    \hat{\mathbf{H}} = \frac{1}{m}\mathbf{X}^T\mathbf{X} - \hat{\mathbf{D}},
\end{equation}
where $\MX$ is the (uncentered) genotype matrix and $\hat{\mathbf{D}}$ is a matrix of diagonal entries ``correcting'' heterozygosities and $m$ is the number of SNPs.

In particular $d_{ij} = 0 $ for $i \neq j$ and, 
\begin{equation}
    d_{ii} = d_i = \frac{1}{m}\sum_{k=1}^m x_{ik}(2-x_{ik}),
\end{equation}
for diploid data. The correction term given by Reich 2009 and Patterson 2012 is different by a factor of four:
\begin{equation*}
    d_{ii}' = \frac{1}{m}\sum_{k=1}^m \frac{x_{ik}(2-x_{ik})}{4} = \frac{d_{ii}}{4}
\end{equation*}.
The difference is explained that they use allele frequencies instead of genotypes, i.e. their approach would use (again for diploid data)
\begin{equation*}
    \hat{\mathbf{H}'} = \frac{1}{m}\frac{\mathbf{X}^T}{2}\frac{\mathbf{X}}{2} - \frac{\hat{\mathbf{D'}}}{4} = \frac{\hat{\mathbf{H}}}{4},
\end{equation*}
and so the parametrizations are equivalent, but will differ by a factor of four.

Thus, (TODO: double-check if rows/columns are aligned, sum should be over SNP)

\begin{subequations}\begin{align}
    h_{ii} &= \frac{1}{m}\sum_{k=1}^m x_{ik}^2 - d_i \\
    h_{ij} &= \frac{1}{m}\sum_{k=1}^m x_{ik}x_{jk} 
\end{align}\end{subequations}
Consider now
\begin{align}
    f_{ij} &= h_{ii} + h_{jj} - 2 h_{ij}\nonumber\\
     &= \frac{1}{m}\sum_{k=1}^m x_{ik}^2 - d_i + \frac{1}{m}\sum_{k=1}^m x_{jk}^2 - d_j - \frac{2}{m}\sum_{k=1}^m x_{ik}x_{jk}\nonumber \\
    &= \frac{1}{m} \sum_{k=1}^m (x_{ik} - x_{jk})^2 - d_i - d_j \\
    &= F_2(i,j)\nonumber
\end{align}

Hence the matrix $\hat{\mathbf{H}}$ can be used to estimate $F_2$-statistics (possibly instead of PPCA?).
The detailed justification of this can be found in \href{https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional}{this statsexchange post  that will need to be adapted}.

Thus, this might be a useful alternative to PPCA to calculate $F$-statistics:

\begin{enumerate}
    \item Calculate $\hat{\mathbf{H}} = \frac{1}{m}\mathbf{X}^T\mathbf{X} - \hat{\mathbf{D}}$
    \item Double-Center $\hat{\mathbf{H}}$: $\mathbf{H}_c = \mathbf{C}\hat{\mathbf{H}}\mathbf{C}$, where $\mathbf{C}$ is a centering matrix
    \item Obtain PCs using an eigendecomposition of $\mathbf{H}_c$: $\MP\MP^T = \mathbf{H}_c$
    \item Calculate $F_2$ from the smaller space $\MP$
\end{enumerate}

\section{Standard errors and effective number of SNPs}
An issue in calculating standard errors for $F$-statistics is that SNP are usually correlated, and so standard variance and standard error calculations will fail.

let us assume $n$ populations can be represented by some population structure model that is parameterized by some covariance matrix $\MX$. We do not observe SNPs, but rather we have a noisy sample $\MG_{[S \times n]}$ at $S$ loci.

Let, as above, denote the data matrix as $\MG$, which is a noisy version of an allele frequency matrix $\MX$, and we assume we can do a PCA on some estimate of $\hat{\MX}$ as 
$\ML\MZ = \hat{\MX}$ where $\ML$ are the orthonormal SNP-loadings and $\MZ$ are the PCs. For example, we could do that using probabilistic PCA or the Cabreros-Storey-PCA

We are interested in statistics of the form 
\begin{equation}
    F_{ij} = (X_i - X_j)^2
\end{equation}
which can be estimated from $\MG$ using the unbiased estimator of Patterson et al.
\begin{equation}
    f_{ij} = \frac{1}{S}\sum_{s=1}^n (g_{si} - g_{sj})^2 - H_i - H_j.
\end{equation}

alternatively, we can also obtain an estimate from the decomposition of  $\hat{\MX}$ as
\begin{equation}
    p_{ij} = \frac{1}{S}\sum_{p=1}^n (z_{si} - z_{sj})^2,
\end{equation}
this sum is over PCs. This estimator has thus the advantage that it can be computed a lot faster since typically $n \ll S$, and if $X$ is low-rank we can even truncate the sum

An issue is the calculation of the standard error of $f_{ij}$ and $p_{ij}$. A simple estimator is 
\begin{equation}
    \sigma_p = \sqrt{\frac{\operatorname{Var}(p_{ij})}{S} }
\end{equation}
a problem with $\sigma_p$ is that SNPs are not independent, and so the variance estimates are underestimated. For this purpose, a block-jackknife can be used.

We block data using a vector $b$, s.t $b_i = j$ means that the $i$-th SNP is in block $j$. For each block, we then have the pseudovalue 
$$\Tilde{f}^{(j)} = \frac{1}{m_j} \sum_s I[b_s =j] (x_{si} - x_{sj})^2  $$
where $I[\cdot]$ is an indicator and $m_j$ is the number of entries in block $j$.

then
\begin{equation}
    \sigma_f' = \sqrt{ \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}( \Tilde{f}^{(j)} - f)^2  \right] }
\end{equation}

This motivates the effective number of SNPs, 
\begin{equation}
    S_e = S \left(\frac{ \sigma_f}{\sigma_f'} \right)^2
\end{equation}

which gives the number of pseudo-independent observations.

\paragraph{simplifying block-JK}
Writing
$$f = \frac{1}{n}\sum_s (x_{si} - x_{sj})^2$$


\begin{align}
    \sigma_f'^2 &=  \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}( \Tilde{f}^{(j)} - f)^2  \right] \\
    &= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \left[ \frac{1}{m_j} \sum_s I[b_s =j] (x_{si} - x_{sj})^2\right] - f\right)^2  \right]\\
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \left[ \frac{1}{m_j} \sum_s I[b_s =j] (x_{si} - x_{sj})^2\right] - \left[\frac{1}{n}\sum_s (x_{si} - x_{sj})^2\right] \right)^2  \right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \sum_s \left[ \frac{1}{m_j} I[b_s =j] (x_{si} - x_{sj})^2 - \frac{1}{n} (x_{si} - x_{sj})^2\right] \right)^2  \right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \sum_s \left[ \frac{  I[b_s =j] n (x_{si} - x_{sj})^2 - m_j(x_{si} - x_{sj})^2 }{n m_j}   \right] \right)^2  \right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}\frac{1}{n^2m_j^2} \left(   \sum_s \left[  (x_{si} - x_{sj})^2 ( I[b_s =j] n  - m_j)    \right] \right)^2\right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}\frac{1}{n^2m_j^2} \left(   \sum_k (z_{ki} - x_{kj})^2 \underbrace{ \left\{n \sum_{s \in b}  l_{sk}^2  - m_j \right\}}_{ v_{bk}} \right)^2\right]\\    
\end{align}
Due to the indicator, this does not simplify easily.




This procedure can be run for any statistic. In particular, we could also do that on some estimates of $X_1$, i.e. columns of the denoised data:

\begin{align}
    \sigma^2_{x} &= \frac{1}{n} \sum_s (x_{s} -\bar{x})^2  \\
    &= \frac{1}{n} \sum_s \left( \sum_k  l_{sk} [z_{k1} - \bar{z}] \right)^2\\
    &= \frac{1}{n} \sum_s \left( \sum_k  l_{sk}^2 [z_{k1} - \bar{z}]^2 + 2\sum_k\sum_{k'}  l_{sk}l_{sk'} [z_{k1} - \bar{z}][z_{k'1} - \bar{z}] \right)^2\\  
    &= \frac{1}{n} \left [ \sum_k [z_{k1} - \bar{z}]^2 \left( \sum_s  l_{sk}^2\right)\right]  + \frac{2}{n} \left[ \sum_k\sum_{k'}  [z_{k1} - \bar{z}][z_{k'1} - \bar{z}] \left( \sum_s  l_{sk}l_{sk'}\right)\right ]  \\
    &= \frac{1}{n} \left [ \sum_k [z_{k1} - \bar{z}]^2 \right]
\end{align}

where we use the PCA-decomposition
\begin{equation}
    x_{s1} = \sum_k l_{sk} z_{k1}
\end{equation}
and the fact that the $l$ are orthonormal.

\section{Squared standard error/variance}
For  normal RV $X,Y$, $\COV{X^2}{Y^2} = 2 \COV{X}{Y}^2$.

Let $X_i^2$ be a vector of squared allele frequencies for individual $i$. We can think of $X_i$ as the location of the individual in an $s$-dimensional coordinate system, or, equivalently, the distance from an arbitrary point. This could either be the reference genome (represented by the zero vector), or the data centroid (for mean-centered SNPs). In either case, $ \sum_s x_{is}^2$ is the squared distance from that reference point, and $\sigma_i = \frac{1}{S} \sqrt{\sum_s x_{is}^2}$ is an estimate of the standard deviation per dimension. 


Alternative: Following the probabilistic PCA model, there are three sources of variation in the data:
\begin{enumerate}
    \item Biological variation due to shared ancestry. These are the principal components $\MP$ we aim to estimate
    \item Sampling error due to a finite number of loci. These are given by the $\ML$. Each entry of $\ML$ is normally distributed, but rows are typically not independent. Furthermore, for genetic data they are typically not independent.
    \item Sampling error due to finite samples from each population. These are given by the heterozygosities
\end{enumerate}

\begin{align}
x_{s} &= \sum_k \ML_k z_{sk} + \mu + \epsilon_s\\
x_{s} - x_{t} &= \sum_k \ML_k (z_{sk} - z_{lk}) +  \epsilon_s -  \epsilon_t\\
(x_{s} - x_{t})^2 &=  \left(\sum_k \ML_k (z_{sk} - z_{lk} ) +  \epsilon_s - \epsilon_t\right)^2\\
&= \sum_k(z_s - z_l)^2 + \epsilon_s^2 + \epsilon_t^2
\end{align}
because the error terms are independent of the $z_s$, and $\ML$ is an orthonormal basis such that $\sum_k {L_k}^2=1$ and $\sum_{k, k'} {L_k L_k'} = 0$

This shows that the \emph{sampling errors} are simply independent of the $F$-stats, and can be obtained e.g. from the heterozygosities (following Chen \& Storey, as in van Waaij et al). They also correspond to the ``correction terms''.

This is just o



\section{Introduction-not used }
A crucial first goal in any population genetic analysis of a natural population is to describe the observed genetic variation in the system. A major topic of population genetics is to link these patterns of variation with the underlying processes, including genetic drift, migration and selection.

Admixture followed by drift can influence and shape the genetic diversity of populations. An important goal of many population genetic studies is to identify the admixture events that have left their mark as patterns of variation in human populations. 

Genetic variation in a natural population is shaped by many processes, including genetic drift, selection, and admixture. Admixture followed by drift is one of the most prevalent process that shaped human genetic diversity 



\bibliographystyle{plain}
\bibliography{references.bib}





\end{document}


