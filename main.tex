\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[super]{nth}

\usepackage{lineno}
\usepackage[
singlelinecheck=false
]{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{csvsimple}
\usepackage[section]{placeins}
\usepackage{lineno}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage[breaklinks]{hyperref}
\usepackage{microtype}

\title{Estimating F-statistics in a probabilistic PCA space}
\author{Divyaratan Popli, Benjamin M. Peter}
%\date{5 August 2021}
\linenumbers

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\newcommand{\BZ}{\mathbf{Z}}
\newcommand{\BD}{\mathbf{D}}
\newcommand{\BN}{\mathbf{N}}
\newcommand{\BH}{\mathbf{H}}
\newcommand{\Btheta}{\pmb{\theta}}


\hbadness=99999
\begin{document}

\maketitle


\begin{abstract}

\noindent Principal component analysis (PCA) and F-statistics are routinely used in population genetic and archaeogenetic studies. However, these are closely related analyses and reveal the same biological signal. Here, we present a statistical framework to combine them into a joint analysis. In particular, we discuss the differences of probabilistic PCA and ordinary PCA, and show that F-statistics are more naturally interpreted in a probabilistic PCA framework. We also show that individual-based F-statistics can be accurately estimated from probabilistic PCA in the presence of large amounts of missing data. We present preliminary results using simulations and published data, and show that this joint estimation framework addresses limitations of estimating F-statistics and PCA independently.

\end{abstract}

\section{Introduction}

Admixture between previously isolated populations is common in nature, and affects the patterns of genetic diversity. From these patterns of genetic diversity, one can reconstruct the past events of admixture.  

\subsection{Overview of methods to study admixture}
There are several methods available to make inferences about admixture events that explain the observed genetic diversity. These methods can be classified as local or global ancestry based. Local ancestry based methods infer ancestry at each locus and reveal recent history of each individual, but have low power to infer events in deep past \cite{vi_genome-wide_2023, brisbin_pcadmix_2012, price_sensitive_2009, sankararaman_estimating_2008}. Commonly used methods to infer global ancestry are Principal Component Analysis (PCA) \cite{mcvean_genealogical_2009}, MDS \cite{wang_comparison_2009}, STRUCTURE \cite{pritchard_inference_2000} and ADMIXTURE \cite{alexander_fast_2009}. These are powerful methods to infer admixture, but can be difficult to interpret since they do not provide model comparisons or formal tests of admixture.  

\subsection{F-statistics}
F-statistics are a popular way to infer global ancestry. They provide an intuitive and powerful way to test hypothesis of admixture by measuring the genetic drift shared between two, three, or four populations \cite{patterson_ancient_2012, peter_admixture_2016}. In this framework, the null model is represented by a tree connecting the populations, with the branch lengths denoting the shared drift, and the null hypothesis can be rejected when the variation in observed data can not be explained by a tree-like structure. It is worth noting that the measure of branch lengths here is based on the allele frequencies in the populations of interest. These allele frequencies can be estimated accurately when there are large number of samples available for each population. However, the sampling bias becomes important when sample sizes are small, and the branch lengths estimated from population allele frequencies should then be corrected \cite{patterson_ancient_2012, peter_admixture_2016}. We provide a detailed definition of F-statistics, and the correction terms for sampling bias in section YY.

\subsection{Estimation of population allele frequencies}
We describe two issues in accurate estimation of population allele frequencies:

1. Humans may not fit into well-differentiated discrete populations, except in cases where the populations have been isolated due to geographical features \cite{novembre_genes_2008}. The estimation of population allele frequencies depends on the assignment of individuals to discrete populations, and may be affected by miss-assignment especially when few samples are available. 

2. Missing data in some individuals for certain sites can make it difficult to get accurate allele frequency estimates at those sites. One commonly used solution to this problem is to filter out all the sites with missing data. However, this may make the number of sites available for F-statistics quite small. E.g., for 100 individuals with $10\%$ randomly missing sites, the available number of sites after filtering out positions with missing data would be $\approx26$ out of a total 1,000,000 sites. 

\subsection{PCA and F-statistics}

PCA is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation while retaining as much of the original variance as possible. It achieves this by finding orthogonal axes, called principal components (PC's), that capture the maximum variance in the data. It is commonly used to understand structure and admixture between populations. 

Issue 1 in the previous section can be addressed by estimating F-statistics from PCA \cite{peter_geometric_2022}. We describe a formal relation between PCA and F-statistics (refer to \cite{peter_geometric_2022} for a detailed derivation) in section YY, but here we point out that a joint framework to estimate PCA and F-statistics not only addresses the issue of population assignment in F-statistics, but also can potentially resolve some issues with the interpretations of PCA. Studies using PCA generally use specific PC's to visualize population structure or admixture, and the choice of the PC's used can be quite subjective \cite{elhaik_principal_2022}. Estimation of F-statistics from PCA quantifies, using all the important PC's, what the researcher has visualized using seemingly arbitrary PC's.

However, a limitation with such a framework is that it is possible to define F-statistics in terms of PC's only when allele frequencies are known, and need not be estimated. This is due to the fact that PCA does not filter the noise in the data due to sampling. In addition, missing data can affect the computation of PC's, and subsequently F-statistics.

\subsection{Probabilistic PCA and Latent Subspace Estimation (LSE)}

We can filter the noise due to sampling by using probabilistic PCA (PPCA), which is an extension of PCA that incorporates a probabilistic framework \cite{tipping_probabilistic_nodate}. PPCA models the observed data as generated by a linear transformation of a lower-dimensional latent space $W$ with added Gaussian noise $\Psi$. Another way to filter out the sampling noise is by removing binomial sampling noise from the covariance matrix of the unnormalized data before estimating the PC's \cite{van_waaij_evaluation_2023,chen and storey, cabreros_likelihood-free_2019}. This is referred to as Latent Subspace Estimation (LSE) \cite{cabreros_likelihood-free_2019}. We give a detailed explanation of both the methods in section YY. In practice, PCA, PPCA and LSE differ in the way that they model the noise in the data due to sampling. Fig. 1 shows that PPCA models the sampling noise as multivariate normal, LSE assumes and removes binomial noise, while PCA does not model the sampling noise. In addition, PPCA provides a natural framework to estimate PC's when some or even all of the samples have certain sites missing at random \cite{tipping_probabilistic_nodate}, thus providing a solution to issue 2. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/pca_all_genetic.png}
    \centering
    \caption{Comparison of PCA, PPCA and LSE: We simulated 100 individuals, and compared the top eigenvalues obtained from different methods.}
    \label{fig1:pca_ppca}
\end{figure}


We would point out here that in ancient DNA studies PCA is sometimes used as quality-control step by constructing PC's using high-quality samples, and projecting the low-quality samples which may be from the same populations or even the same individuals as the high coverage samples. In the presence of contamination from present-day people, reference bias, ascertainment bias or batch effects, the projected sample may not overlap with an identical high-coverage sample. These biases and issues are not resolved with PPCA/LSE either, since PPCA only models sampling noise. 


\subsection{F-statistics with PPCA/LSE}
In this study, we develop a statistical framework to estimate F-statistics between individuals in a PPCA framework. We show that PPCA explicitly models the error due to the sampling bias in allele frequencies. In addition, we demonstrate that PPCA based framework is not sensitive to random missing data, and so it can be used to visualize individuals in PCA-space without having to project lower quality samples.

We explain that PPCA provides a natural framework to estimate F-statistics with small sample-size and missing data. We show formal hypothesis tests for admixture and compare our results to admixtools2 \cite{maier_limits_2022} on simulations. Finally we show the use of this framework on published datasets from neolithic Haak et al. and upper Paleolithic humans (Mateja). 


\section{Theory}

\subsection{F-statistics}
We follow the original notation \cite{patterson_ancient_2012}, and write the statistical estimates from empirical data as $f_2$, $f_3$ and $f_4$, and denote the theoretical values that depend only on the phylogenetic tree and the ascertainment scheme as $F_2$, $F_3$ and $F_4$. The three F-statistics are defined in terms of population allele frequencies as follows:

\begin{align}\label{eq:f_intro}
F_2(X_1,X_2) &= \frac{1}{S}\sum_{s=1}^S(x_{1s} - x_{2s})^2\nonumber\\
F_3(X_1;X_2,X_3) &= \frac{1}{S}\sum_{s=1}^S(x_{1s} - x_{2s})(x_{1s} - x_{3s})\nonumber\\
F_4(X_1,X_3;X_2,X_4) &= \frac{1}{S}\sum_{s=1}^S(x_{1s} - x_{2s})(x_{3s} - x_{4s})\nonumber\\
\end{align}

$F_2(X_1,X_2)$ is interpreted as the branch length between populations $X_1$ and $X_2$ (fig S2 a) and it measures the expected amount of drift that occurred between $X_1$ and $X_2$. $F_3(X_1;X_2,X_3)$ represents the amount of drift that occurred on the branch connecting $X_1$ to the common ancestor node of $X_2$ and $X_3$ (fig S2 b). When $X_1$ is admixed between $X_2$ and $X_3$, $F_3(X_1;X_2,X_3)$ may be negative, and hence this is used as a test for admixture\cite{peter_admixture_2016, patterson_ancient_2012}. $F_4(X_1,X_3;X_2,X_4)$ represents the covariance between shared drifts between X1,X2 and X3,X4. This would be represented by the internal branch between the common ancestor nodes of $X_1$, $X_2$ and $X_3$, $X_4$ (fig S2 c). $F_4$ statistic, with a different permutation of the populations can be used as test of admixture. $F_4(X_1,X_2;X_3,X_4)$ is expected to be 0 if $X_1$, $X_2$, $X_3$, $X_4$ are related to each other by a tree (fig S2 d). However, a large positive or negative value would suggest a departure from the null model of treeness due to gene flow between X1 and X3 or X2 and X3 respectively.

Since $F_2$ represents the branch length between a pair of populations, we can write $F_3$ and $F_4$ as linear combination of $F_2$'s.

\begin{align}\label{eq:f3_f4}
F_3(X_1;X_2,X_3) &= \frac{1}{2} [F_2(X_1,X_2) + F_2(X_1,X_3) - F_2(X_2,X_3)]\nonumber\\
F_4(X_1,X_2;X_3,X_4) &= \frac{1}{2} [F_2(X_1,X_3) + F_2(X_2,X_4) - F_2(X_1,X_4) - F_2(X_2,X_3)]\nonumber\\
\end{align}

However, as explained in the introduction, the estimation of $F_2$ may be biased when population allele frequencies are estimated from small sample sizes. Correction to sampling bias has been described in literature, and here we present the correction terms without derivation \cite{peter_admixture_2016, patterson_ancient_2012}.

\begin{align}\label{eq:f2_correction}
F_2(X_1,X_2) &= \sum_{s=1}^S[(x_{1s} - x_{2s})^2 - \frac{x_{1s}(1-x_{1s})}{n_{1s}} - \frac{x_{2s}(1-x_{1s})}{n_{2s}}]
\end{align}

Sampling bias affects the estimates of $F_3$ as well but not $F_4$. We state the correction term for $F_3$ here without derivation \cite{peter_admixture_2016, patterson_ancient_2012}.

\begin{align}\label{eq:f2_correction}
F_3(X_1;X_2,X_3) &= \sum_{s=1}^S(x_{1s} - x_{2s})(x_{1s} - x_{3s}) - \frac{x_{1s}(1-x_{1s})}{n_{1s}}
\end{align}

Assuming that the population allele frequencies are known, we can think of F-statistics in terms of Euclidean distances in an allele frequency space \cite{oteo-garcia_geometrical_2021}. In this framework, each population can be represented as a vector in a multi dimensional allele frequency space, and $F_2(X_1, X_2)$ can be calculated as squared Euclidean distance between the vectors (fig. S2 e). $F_3(X_1;X_2,X_3)$ is then a dot product of the vectors $\Vec{x1} - \Vec{x2}$ and $\Vec{x1} - \Vec{x3}$ (fig. S2f). $F_4(X_1,X_3;X_2,X_4)$ is a dot product of vectors $\Vec{x1} - \Vec{x3}$ and $\Vec{x2} - \Vec{x4}$ (fig. S2 f), and $F_4(X_1,X_2;X_3,X_4)$ is a dot product of vectors $\Vec{x1} - \Vec{x2}$ and $\Vec{x3} - \Vec{x4}$ (fig. S2 g). 

\begin{align}\label{eq:f_intro}
F_2(X_1,X_2) &= \frac{1}{S}||\Vec{x_{1}} - \vec{x_{2}}||^2\nonumber\\
F_3(X_1;X_2,X_3) &= \frac{1}{S}||\vec{x_{1}} - \vec{x_{2}})\cdot(\vec{x_{1}} - \vec{x_{3}})||\nonumber\\
F_4(X_1,X_3;X_2,X_4) &= \frac{1}{S}||(x_{1} - x_{2})\cdot(x_{3} - x_{4})||\nonumber\\
\end{align}


\subsection{PCA and F-statistics}

This geometric framework provides an alternate way to understand the properties of F-statistics \cite{oteo-garcia_geometrical_2021}. However, many population genetic studies use many SNPs (in the order of a million), and it is not possible to visualize population vectors in such a high dimensional space. Peter at al. showed that one can do dimentionality reduction on such datasets with PCA, and use the top PC's to estimate F-statistics \cite{peter_geometric_2022}.

Let us assume M populations and S snps, such that our dataset X has the dimension $[M \times S]$, and each entry of X is an allele frequency $\in$ [0,1]. PCA allows us to project this $[M \times S]$ high dimensional data on a lower dimensional subspace $[q \times M]$. $q = M-1$ represents the case where we retain all the PC's, and thus PCA only rotates X. However, in practice we often only need few PC's (q<<M) to explain most variation in the populations \cite{peter_geometric_2022}. A common algorithm to estimate PC's is Singular Value Decomposition (SVD). For this approach we first mean-center X, and then decompose X into U, $\Sigma$, and $V^T$.

$$Y = CX = (U\Sigma ) V^T = WL,$$

where C is the centering matrix such that $C = I - (1/M)I$. We perform SVD to decompose Y into a product of $W = U\Sigma$ and $L = V^T$. In the context of PCA, $W_{MxM}$ is a matrix of principal components and contains information about structure, while $L_{MxS}$, also known as SNP loadings contains the contribution of each SNP to each PC. L can be used to identify outlier SNPs that may be potential candidates for selection \cite{gower_distance_1966}. 

In section YY we showed that F-statistics can be written as the dot product of vectors in an allele frequency space. Dot product is invariant to PCA, which is just a rotation of the dataset to axes of largest variation. Therefore, we can calculate F-statistics from the PC's directly \cite{peter_geometric_2022}:

\begin{align}\label{eq:f_intro}
F_2(X_1,X_2) &= \sum_{s=1}^S(x_{1s} - x_{2s})^2\nonumber\\
&= \sum_{s=1}^S(x_{1s} - \mu_s)(x_{jl} - \mu_s) = F_2(Y_1,Y_2)\nonumber\\
&= \sum_{s=1}^q(w_{1s} - w_{2s})^2 = F_2(W_1,W_2)\nonumber\\
\end{align}

$F_3$ and $F_4$ can then be written in terms of $F_2$, and remain invariable to change in axes. For many applications, we only need few PC's with highest variation to approximate F-statistics \cite{peter_geometric_2022}. 

\subsection{PPCA}

PPCA is a dimensionality reduction technique that extends the classical PCA by introducing a probabilistic framework. It is a statistical model that assumes that the observed data is generated from a lower-dimensional latent space.

In classical PCA, the goal is to find a linear transformation (rotation) of the data that captures the maximum amount of variance in the original dataset. However, PCA does not provide a probabilistic interpretation of the data and does not explicitly model noise or uncertainty.

PPCA, on the other hand, introduces a probabilistic generative model. It assumes that the observed data points are generated by a linear transformation of a lower-dimensional latent space, with an additional Gaussian noise term. The latent variables capture the underlying structure or patterns in the data, while the noise accounts for variability due to sampling error.

Mathematically, the PPCA model can be represented as follows \cite{tipping_probabilistic_nodate}:

Latent variable model:
Latent variables: $Z \sim N(0, I)$, where Z is a S-dimensional latent variable, and I is the identity matrix.

Latent-to-observed mapping: $X = WZ + \mu + \Psi$, where X is the observed data, W is a M x q matrix of linear mappings, $\mu$ is the mean of the observed data, and $\Psi$ is a Gaussian noise term.

Prior distributions:

Prior on the latent variables: $P(Z) = N(0, I)$

Prior on the noise term: $P(\Psi) = N(0, \sigma^2I)$, where $\sigma^2$ is the variance of the noise.

Likelihood function:
$p(X|Z, W, \mu, \Psi) = N(X|WZ + \mu, \sigma^2I)$

The goal of PPCA is to estimate the parameters of the model, namely W, $\mu$, and $\sigma^2$, given the observed data. This is typically done through the standard expectation-maximization (EM) algorithm or maximum likelihood estimation (MLE).

Probabilistic PCA provides a probabilistic framework that offers several advantages over classical PCA, including the ability to model uncertainty, and handle missing data. 

\subsection{LSE}

LSE is a dimensionality reduction technique quite similar to PCA, with the difference that LSE accounts for the heteroscedasticity in the data \cite{chen_consistent_2015}. From the data matrix X, we calculate $\delta_j = \frac{1}{S}\sum_i2x_{ij} - x_{ij}^2$. We define D as a diagonal matrix with $j^{th}$ entry as $\delta_j$. We then calculate the eigenvalues of G = $\frac{1}{S}X^TX - D$. The eigenvectors of G then span the latent subspace of L , and the smallest M-q eigenvalues converge to 0 for large M \cite{cabreros_likelihood-free_2019}. 




\section{Results}

We evaluate the performance of PCA, PPCA and LSE frameworks and compare to that of admixtools 2 using simulations, and a Neandertal dataset. 

\subsection{Evaluation on simulations}
We simulated 10 populations with 10 individuals each using slendr \cite{petr_slendr_2022} with the standard mutation rate $10^{-8}$ per base per generation, recombination rate $10^{-8}$ per base per generation, and a generation time of 30 years. Fig. 2 shows the split times and migration events. 

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{inkscape/sim1.png}
    \centering
    \caption{Simulated trees for testing our estimation of F-statistics. Split time for each node in generations and the admixture branches are shown in dashed lines. The red arrow represents a migration event from Pop3 to Pop2.}
    \label{fig2:sim}
\end{figure}

We use equation 6 to express $F_2$ for a pair of populations as a squared Euclidean distance between them (see Fig. S2). We estimate the PC's using PCA, PPCA and LSE, and compare F-statistics estimated from these methods to the true theoretical value of the statistic obtained from branch lengths in slendr \cite{petr_slendr_2022}. We first do a comparison between $F_2$'s estimated from PCA, PPCA and LSE with different number of PC's used (Fig. S1). Here, we use 10 individuals in each population, and we see that all three methods are not sensitive to the number of PC's used, as long as the number of PC's is higher than 7. Estimate of F2 with PCA is only slightly higher than PPCA and LSE, since population size of 10 diploid individuals is large enough to ignore sampling error correction. We next look at a comparison between the three methods using only one individual in each population (Fig. S2). Here we observe that $F2$ estimates from PPCA and LSE are quite close to the true value at all cases with $PCs \geq 7$. However, estimates from PCA get higher with more PC's used. This is expected from theory (section XX), since PCA does not account for sampling bias.

In the rest of the analyses, we exclude PCA, and compare PPCA and LSE with 8 and 12 PC's to admixtools2, which is a popularly used tool in population genetics \cite{maier_limits_2022}. We first compare these methods in an ideal scenario, where each population has 10 individuals, and there is no missing data. In this case we find that the three methods perform well, and get F-statistics close to the truth (fig. S3). We then sub-sample only one individual from each population, and find individual-based F-statistics (fig. 1). We observe that in this case both PPCA and LSE- based frameworks perform atleast as well as ADMIXTOOLS 2. The mean estimate from each method is close to the true value, however, the error bars are lower for PPCA compared to ADMIXTOOLS 2, specially for Pop1 and Pop2, which have low split-times. This is an expected result, since ADMIXTOOLS 2 has access to only one individual from each population, while PPCA and LSE utilize the information about structure in the populations from the PC's.       

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/simfiles/Ne1000/split_times1000/npop10_nind100/plots_8_12/mu0.05_f2_plot_slides_1ind.png}
    \centering
    \caption{Comparison of PPCA and LSE to ADMIXTOOLS 2 using population genotypes from ten individuals in each population.}
    \label{figS2:pc_scale}
\end{figure}

We next address the issues listed in Introduction section XX. Issue 1 is about estimation of F-statistics when population assignment is difficult, especially when few samples are available. We show that this can be resolved with individual-based F-statistics. In our simulations, we label each individual as a different population, and we sample one ancient individual from each population  to calculate F-statistics. Fig. YY shows that all three methods on an average are close to the true values, but admixtools2 shows higher variability in the estimates. This is expected, since both PPCA and LSE do not utilize the population labels, and get the information about structure and admixture from all samples, even though the F-statistics are calculated using only one individual from each population. On the other hand, admixtools 2 has only one individual from each population to assess structure / admixture. 
Test of admixture for F3/F4

Finally, we evaluate the estimation of these methods when there is random missing data. Our implementation of PPCA on missing data is inspired from EMU \cite{meisner_large-scale_2021}, and is described in Methods section YY. We see that PPCA is not affected by missing data, while admixtools 2 results are inflated (Fig. 3).

\subsection{Evaluation on neandertal dataset}


\section{Methods}

\subsection{PPCA implimentation}

\subsection{PPCA implementation for missing data}


\section{Discussion}

We show that F-statistics estimated from LSE using all eigenvalues is same as F-statistics calculated with admixtools 2, and when we use specific eigenvalues, LSE and PPCA show similar results. We can estimate F-statistics using a PPCA framework even in the presence of random missing data. We compare the error model 




\section{Supplementary Figures}


\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/supplementary/Ne1000_split_times1000_npop10_nind100_mu0_f2_plot_scale_test.png}
    \centering
    \caption{Comparison of PCA, PPCA and LSE using population allele frequencies of 10 individuals in each population. X-axis shows the number of PC's used for the three methods, ranging from 2 to 50.}
    \label{figS1:pc_scale}
\end{figure}

\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/supplementary/Ne1000_split_times1000_npop10_nind100_mu0_f2_plot_scale_test_ind.png}
    \centering
    \caption{Comparison of PCA, PPCA and LSE using population genotypes from one individual in each population. X-axis shows the number of PC's used for the three methods, ranging from 2 to 50.}
    \label{figS2:pc_scale}
\end{figure}


\begin{figure}[ht!]
    \includegraphics[width=16.5cm]{plots/simfiles/Ne1000/split_times1000/npop10_nind100/plots_8_12/mu0.05_f2_plot_slides.png}
    \centering
    \caption{Comparison of PPCA and LSE to ADMIXTOOLS 2 using population genotypes from ten individuals in each population.}
    \label{figS2:pc_scale}
\end{figure}




\section{old stuff}
\subsection{PCA}

One way to estimate F-statistics 

the estimation of allele frequencies can also be affected by large amounts of missing data. Since F-statistics are dependent on the ascertainment scheme, random missing data reduces the number of overlapping sites greatly.
PCA is a powerful method to visualize population structure but it can be difficult to interpret \citep{cavalli-sforza1993, novembre_stephens_2008, degiorgio_rosenberg2013}. In particular, the population structure in PCA is a function of expected pairwise coalescence times \citep{mcvean_genealogical_2009}, and thus is not explicitly tied to a particular scenario; different histories may yield similar or identical PCAs. 

Thus, parameter estimation, model comparisions and formal tests of admixture are usually not carried out in PCA. 

(add section here introducing the general idea of PCA, explain how it deals with error/noise  and difference between probabilistic and regular PCA. Possibly also explain how people use PCA as a QC-step to detect batch effects, and how that compares with PCA used for population structure (i.e. Ainash' question from your talk))

F-statistics are a useful tool to quantify population structure, and provide tests for admixture. Hence, a common pipeline in many population genetic studies is to analyze PCA plots to look for visible patterns that could be due to past admixtures, followed by formal test with F-statistics \cite{lazaridis_ancient_2014,lazaridis_genomic_2016}. F-statistics use population allele frequencies to test for admixture, and hence the accuracy of these tests is limited by the accuracy of allele frequency estimates \cite{peter_admixture_2016}. Whereas F-statistics estimates are robust when there are enough high quality samples, we show that low number of samples and missing data decrease the accuracy of these tests.

A major issue when working with ancient DNA is low number of individuals and missing data. This issue is exacerbated due to difficulty in assigning some of the individuals to discrete populations. Especially in the case of humans samples, we know that the genetic samples do not strictly belong to discrete populations, but form a continuous spectrum in the allele frequency space \cite{oteo-garcia_geometrical_2021}. Hence, it is a step in the right direction to think of a method to estimate F-statistics in a structure-aware framework. The easiest way to think about this is using PCA. PCA does not require assignment of individuals to discrete populations, and it has been shown that F-statistics can be estimated conveniently from distance between populations on PCA space \cite{peter_geometric_2022}. However, PCA distances are inaccurate when population sizes are small since PCA does not explicitly model sampling bias in the allele frequencies (see section XX). In addition, PCA is sensitive to missing data, and this makes it difficult to work with ancient DNA. 



In this study, we develop a statistical framework to estimate F-statistics between many populations in a probabilistic PCA framework.  that probabilistic PCA (PPCA) explicitly models the error due to the sampling bias in allele frequencies. In addition. we demonstrate that PPCA based framework is not sensitive to random missing data, and so it can be used to visualize individuals in PCA-space without having to project lower quality samples.

Finally, we show that PPCA provides a natural framework to estimate F-statistics with small sample-size and missing data. We show formal hypothesis tests for admixture and compare our results to admixtools2 cite{admixtools paper} on simulations. Finally we show the use of this framework on published datasets from neolithic Haak et al. and upper Paleolithic humans (Mateja). 





\section{older stuff}
There are several tools available to understand genetic diversity, and can be classified as the tools that make minimal assumptions to summarize population structure, and the tools that infer demographic parameters. Former set of tools includes f-statistics \cite{patterson_ancient_2012}, PCA \cite{mcvean_geneaylab=""logical_2009}, MDS \cite{wang_comparison_2009}, Structure \cite{pritchard}, Admixture  
- There are different tools to study diversity: 1) tools that make minimum assumptions like fstatistics, PCA, MDS, Structure, Admixture, 2) tools that can be used to infer demographic parameters.
- Many studies use PCA followed by f-statistics in their pipline. Peter et al., show that these analyses reveal the same biological signal, and can be done jointly.
- This framework has limitation: population allele frequencies are not known.
- Here we present an approach based on pPCA, and show that it is a more natural framework since it can take in account the errors associated with allele frequency estimation.

\subsection{Theory}
- f-statistics, and the sampling error terms in f2.
- PCA and relation to f-statstics.
- pPCA/PCA1 (Waaij et. al.) as a framework for dimentionality reduction taking in account the sampling error.

\subsection{results}
- Advantage in estimating PCs: Fig.1: PCA plot with standard errors

- Advantage in estimating f-statistics: We compare f-statistics from pPCA, PCA1, PCA, admixtools2 in terms of accuracy (and speed?). Fig.2: point estimates of f2's for slendr simulations where we have both ancient and modern populations. Fig.3: Examples of f4 test of treeness with different cases of migrations, we can also show a case of f3 test of admixture.

- Application to a published dataset (Fig.4)

\subsection{Discussion}
- One key advantage of this framework is that both point estimates and standard errors for PCA and fstatistics are estimated together in a consistent way.
- This is a step towards solving the issue with the assumption of discrete populations.
- This would be quite useful for cases where assigning individuals to populations can be difficult.
- Future work: 1) A faster way of estimating standard errors. 1) to get uncertainty from snp loading. 2) Hypothesis testing 

- This approach could also help with missingness as shown by Meisner et al. 
- pPCA makes it easy to analyse modern and ancient data together without having to project samples.

\subsection{Methods}
- Simulation method and parameters used


\section{Abstract}

Studies of genetic variation now routinely include data from thousands of individuals representing complex historical and temporal structure. Understanding and modeling patterns of genetic variation between large numbers of individuals and populations is thus a key challenge in the usage of genetic data to answer questions about evolutionary history. 
Principle Component Analysis (PCA) and F-statistics sensu Patterson are both widely used for this purpose, but are usually analyzed dis-jointly. Here, we present a new framework based on 
probabilistic PCA to jointly estimate principal component and F-statistics from large panels of data. A key advantage of our approach is that we can calculate individual-based F-statistics efficiently, and so population assignments become a result rather than an a priori assumption. Furthermore, probabilistic PCA provides a natural framework for incorporating missing data, a common issue in ancient DNA analyses. Taken together, our results greatly simplify the analysis of large population genetic data sets, and allow for fast data exploration and statistical testing in a unified and consistent framework.

\section{Background}

\subsection{Why study genetic diversity}
The genetic diversity of human populations has been shaped by historical and environmental factors over hundreds of thousands of years. Therefore, a key objective of population genetics is to analyze the observable variations and patterns in order to understand and reconstruct the demographic and evolutionary history of our species. 

\subsection{The general pipeline}

A general pipeline consists of a method to summarize the data with minimal assumptions. Examples are PCA, MDS, Structure, Admixture. These methods show a qualitative picture, but do not estimate a biologically meaningful parameter. And so, it is difficult to design statistical tests for the results of such methods. However, such qualitative results are generally followed by quantifiable methods based on f-statistics. F-statistics with 2,3, or 4 populations assumes a null-model as a tree-like structure and a deviation from the tree-like structure is represented the alternate model.

\subsection{PCA and fstatistics}
PCA is a method to rotate the dataset in a way that so that the axes that are analysed and plotted are aligned with the dimensions explaining the highest variation in the data. This is a way to do dimensionality reduction, and provides a way to do better data visualization. Ben[2020] showed that PCA and f-statistics are related, and   


\section{Introduction}
\subsection{Why combine pPCA and F-statistics?}

\paragraph{In case of no missing data}

- Calculating f-statistics between populations already assumes clustering of individuals into populations. PCA shows this clustering, but it would be useful to quantify the distance between individuals to filter for individuals that cluster, spot outliers, and identify substructures.

-Faster calculation of f-statistics, when only few PCs are used. Calculation of pPCA may take some time, but afterwards f-statistics is less time taking. And since people anyway do both PCA and f-statistics, overall this would reduce time.

-admixture proportions from f-statistics using pca. We can check if it's more reliable.
\paragraph{In case of missing data}

- In case of admixturegraphs, missing data may reduce total snps (although, missing sites would be less if allele frequencies are calculated from available individuals in populations). pPCA may help in cases with e.g., few ancient individuals from each population with a lot of missing data. 

- In case of ancient DNA, may help to include libraries with missing data instead of projecting them?
\newpage
\section{Practical application ideas}
This is a list of ideas we could pursue. Goal would be to pick a few that would be easiest to accomplish.
\begin{enumerate}
    \item Evaluate whether we use PPCA to calculate individual-based $F$-statistics accurately and fast?
    \begin{enumerate}
        \item in presence of missing data
        \item for multivariate analyses including qpadm/qpgraph
        \item include samples projected onto PCA
    \end{enumerate}
    \item Grouping individuals in populations
    
    \item Can we get confidence intervals on PCA?
        \begin{enumerate}
            \item Resampling SNPs
            \item Resampling individuals
            \item Calculate uncertainty based on SNP loadings
            \begin{enumerate}
                \item For SVD $\mathbf{X} = \mathbf{(UD)V}^T = \mathbf{PV}^T$, we might be able to use the correlation in the entries of $\mathbf{V}$ to estimate the ``effective'' number of SNPs
            \end{enumerate}
        \end{enumerate}
    \item Practical programming
        \begin{enumerate}
            \item Write software tool that jointly computes individidual-based F-stats and PCA
        \end{enumerate}
    \item Data analysis -- find a good data set or scenario to analyze
        \begin{enumerate}
            \item Standard Western Eurasian PCA
            \item Indian data
            \item Some application that Stephan / Wolfgang are working on
        \end{enumerate}
    \item Looking at qpadm/qpwave
        \begin{enumerate}
            \item qpadm projects samples into a subspace made from a subset of samples. As these samples are not orthogonal, this subspace is likely highly non-orthogonal and therefore tricky to work with. Doing PCA before doing qpadm could be helpful. 
        \end{enumerate}        
\end{enumerate}


\section{Projecting onto prob PCA}

For PCA, we can write $X_{[n \times p]} = USV^T$ and the PCs are given by $SV^T$. , and we would project by


\begin{align}
    X &= USV^T \\
    U^T X &= SV^T \\
    XVS^{-1} &= USV^TVS^{-1} \\
    &= U \\
    U^T Y &= Y_{proj}\\
    XVS^{-1}Y &= Y_{proj}
\end{align}
and we can project using $U^TY $ for new data $Y$. 


\newpage
\section{F-tests using Wishart-log-likelihoods}
Consider the $F_4$-statistic
\begin{align}
    F_4(A-B, C-D)&= Cov(A-B, C-D)\\ &= Cov(A, C) + Cov(B,D) - Cov(A,D) - Cov(B,D)\\
\end{align}


\subsection{PCA to Covariance matrix}
If we have a matrix of PCs, $\BP$, then $\mathbf{Y} = \BP\BP^T$ is an estimate of the covariance matrix. Consider the random variables $(A-B)$ and $(C-D)$. If we assume they are jointly normally distributed, then their joint distribution will again be normally distributed with mean zero and covariance matrix 
\begin{equation}
    \bf{X} = \begin{pmatrix} 
    y_{11} + y_{22} - 2 y_{12} & y_{13} + y_{24} - y_{14} - y_{23} \\
    y_{13} + y_{24} - y_{14} - y_{23} & y_{33} + y_{44} - 2 y_{34} &  \\
    \end{pmatrix}
\end{equation}
where the $y$ are the entries of the  covariance matrix obtained from the PCA. Practically, we can either first calculate $Y$ (if we want all $F$-stats), or first subset $\BP$ to the four pops involved.

The off-diagonal elements of $\MX$ are precisely the $F$-statistics we aim to calculate. And thus we set up a test to see whether they are zero.


\subsection{Derivation of the test statistic}
The sampling distribution of a covariance follows a Wishart distribution. This is a $p \times p$ - matrix-valued probability distribution that is parametrized by a degree-of-freedom parameter $n$ and a covariance matrix $\MS$, also of dimension $p \times p$. The simplest way to generate Wishart random variates is, 
\begin{equation}
    \MX = \sum_{i=1}^n Y_i Y_i^T
\end{equation}
where $Y_i \sim N(0, \MS)$. We also have 
\begin{align}
    E[\MX] &= n\MS \\
    mode(\MX) &= (n-p-1) \MS
\end{align}

The log-likelihood of a Wishart Distribution is

\begin{align}
    \log P(\MX | \MS, n) &= -\frac{np}{2} \log(2) -  \frac{n}{2}\log\left|\MS\right| - \Gamma_p\left(\frac {n}{2}\right ) + \frac{n-p-1}{2} \left|\MX\right| -\frac{1}{2}\operatorname{tr}({\MS}^{-1}\MX)\\
    &\propto  -\frac{n}2\log\left|\MS\right|   -\frac{1}{2}\operatorname{tr}({\MS}^{-1}\MX)\nonumber\\
    %&= -\frac{n}2 \log \left( \sigma_{11}\sigma_{22} - \sigma_{12}^2\right) - \frac{1}{2}\frac{\sigma_{22}x_{11} - 2\sigma_{12}x_{12} + \sigma_{11}x_{22}} {\sigma_{11}\sigma_{22} -  \sigma_{12}^2} 
    &= -\frac{n}2 \log \left( \sigma_{11}\sigma_{22} - \sigma_{12}^2\right) - \frac{1}{2}\frac{\sigma_{22}x_{11} + \sigma_{11}x_{22} -2 \sigma_{12}x_{12}} {\sigma_{11}\sigma_{22} -  \sigma_{12}^2} 
\end{align}
where the last step assumes a $2\times 2$ matrix. 
Under H0, we have $\sigma_{12} = 0$, therefore
\begin{align}
    \log P(\MX | \MS_0, n) &= -\frac{n}2 \log \left( \sigma_{11}\sigma_{22}\right) - \frac{1}{2}\left[\frac{x_{11}}{\sigma_{11}}  + \frac{x_{22}}{\sigma_{22}}\right] 
\end{align}
This likelihood is easily separatable and we can estimate $\sigma_{11}$ from $x_{11}$ and $\sigma_{22}$ from $x_{22}$ directly.

The log-likelihood-ratio statistic can then be calculated as 

\begin{align}
    R &= - 2 \log  \left( \frac{P(\MX | \MS_0, n)}{ P(\MX | \MS, n)} \right)  \nonumber\\
    &=2 [\log P(\MX | \MS, n) - \log P(\MX | \MS_0, n) ] \nonumber\\
    &=  n \log \left( \sigma_{11}\sigma_{22}\right) - n \log \left( \sigma_{11}\sigma_{22} - \sigma_{12}^2\right)  \nonumber\\
    &+  n\left[\frac{x_{11}}{\sigma_{11}}  + \frac{x_{22}}{\sigma_{22}}\right] 
    - n\frac{\sigma_{22}x_{11} + \sigma_{11}x_{22} -2 \sigma_{12}x_{12}} {\sigma_{11}\sigma_{22} -  \sigma_{12}^2} 
\end{align}
using the estimates $\sigma_{ij} = \frac{x_{ij}}{n}$ we get
\begin{align}
    R&\approx n \log \left( x_{11}x_{22}\right) - n \log \left( x_{11}x_{22} - x_{12}^2\right) + 2n - 2n  \nonumber\\
     &= n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)
\end{align}
 A $\log(n^2)$ term in each of the log-terms cancels. The statistic $R$ is asymptotically $\chi^2$ distributed with one degree of freedom.

If we instead use the mode $\sigma_{ij} \approx \frac{x_{ij}}{n-3}$:

\begin{align}
    R&\approx n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)
+2 (n-3) \frac{x_{11}x_{22} - x_{12}^2} {x_{11}x_{22} -  x_{12}^2} 
    - 2(n-3) \nonumber\\
     &= n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)
\end{align}

\subsection{Comparison to Cavalli-Sforza \& Piazza, 1975}
The authors propose 
\begin{equation}
    R = \frac{|\MX|}{|\MS|}
\end{equation}
where $|\cdot|$ is the determinant. For $2 \times 2$ matrices,


\begin{align}
|\MX| &= x_{11}x_{22} - x_{12}^2 \\
|\MS| &= x_{11}x_{22} \\
    R &= \frac{ x_{11}x_{22} - x_{12}^2}{ x_{11}x_{22}} =1 - \frac{ x_{12}^2}{ x_{11}x_{22}}\\
    T &= -2 n \log (R) = 2 n \log \left( \frac{ x_{11}x_{22}}{ x_{11}x_{22} - x_{12}^2}\right)
\end{align}

The factor of 2 might be wrong..

\subsection{other statistics}
for $x_{12}$  small, we may further approximate
\begin{align}
         R &= n  \log \left( \frac{x_{11}x_{22}}{x_{11}x_{22} - x_{12}^2}  \right)\nonumber\\
            &= n  \log ( x_{11}x_{22}) - n\log ({x_{11}x_{22} - x_{12}^2} )\nonumber\\
            &= n  \log ( x_{11}x_{22}) - n\log \left[x_{11}x_{22} \left(1  - \frac{x_{12}^2}{x_{11}x_{22}} \right)\right]\nonumber\\
           &=n  \log \left( 1 - \frac{x_{12}^2}{x_{11}x_{22}}  \right) \nonumber\\
           &\approx  n \frac{x_{12}^2}{x_{11}x_{22}} 
\end{align}
This is the coefficient of determination, which is the square of the correlation coefficient
\begin{equation}
    r =  \sqrt{R/n} = \frac{x_{12}}{\sqrt{x_{11}x_{22}}}
\end{equation}

for which we have a $t$-distributed null
\begin{equation}
    r \sqrt{\frac{n-2}{1-r^2}} \sim t(n)
\end{equation}
The Fisher-Transform then yields
\begin{equation}
    \frac{1}{2} \log\left(\frac{1+r}{1-r}\right) = \arctan(r) \sim N(0,1)
\end{equation}
which simplifies to 
\begin{equation}
    \arctan(r) = \frac{1}{2} \log\left(\frac{\sqrt{x_{11}x_{22}} + x_{12}}{\sqrt{x_{11}x_{22}} - x_{12}}\right) 
\end{equation}
This statistic is normally distributed under the null-hypothesis.


in tests with a simple bivariate normal, all of them behaved equally well.


\newpage
\section{Calibrating the standard errors of $F$-stats}
Traditionally, the standard errors of F-stats are estimated using a block-Jackknife approach. However, the block size is usually hard to estimate, and may impact the resulting values.

\subsection{What do the standard errors measure?}
There are two types of uncertainty:
\begin{itemize}
    \item \textbf{sampling uncertainty}, that stems from the fact that we only have a small sample from each population. Because it depends on the sample, we expect these uncertainties to be independent for each sampled population
    \item \textbf{evolutionary uncertainty} there is also uncertainty due to the randomness in evolution. In particular, the realized mean allele frequencies in populations will be different from those expected under some model. 
\end{itemize}

\subsection{Covariance of $F_2$-statistics}
\begin{align}
    K &= Cov( (X_1- X_2)^2, (X_3 - X_4)^2 )  \nonumber\\
    &= Cov(X_1^2, X_3^2) + Cov(X_1^2, X_4^2) + Cov(X_2^2, X_3^2) + Cov(X_2^2, X_4^2) \nonumber\\
    &- 2\left [ Cov(X_1^2, X_3X_4) + Cov(X_2^2, X_3X_4) + Cov(X_3^2, X_1X_2) + Cov(X_4^2, X_1X_2)) \right]\nonumber\\
    &+ 4\left[ Cov(X_1X_2, X_3X_4)\right]\nonumber\\
\end{align}
At the same time, we have

\begin{align}
    F_4^2 &=  Cov(X_1 - X_2, X_3 - X_4) ^2 \nonumber\\
    &= \big[ Cov(X_1, X_3) + Cov(X_2, X_4) - Cov(X_1, X_4) - Cov(X_2, X_3)   \big]^2\nonumber\\
    &= Cov(X_1, X_3)^2 + Cov(X_2, X_4)^2 + Cov(X_1, X_4)^2 + Cov(X_2, X_3)^2  \nonumber\\
    &+2 \big[ Cov(X_1, X_3) Cov(X_2, X_4) + Cov(X_1, X_4) Cov(X_2, X_3)\big] \nonumber\\
    &-2 \big[ Cov(X_1, X_3) Cov(X_1, X_4) + Cov(X_1, X_3) Cov(X_2, X_3)\big] \nonumber\\
    &-2 \big[ Cov(X_2, X_4) Cov(X_1, X_4) + Cov(X_2, X_4) Cov(X_2, X_3)\big] \\
    &= (E[X_1^2] + E[X_2^2])(E[X_3^2] + E[X_4^2]) \nonumber\\
    &+2 \big[ E[X_1X_3]E[X_2X_4] + E[X_1 X_4] E[X_2 X_3]\big] \nonumber\\
    &-2 \big[ E[X_1 X_3] E[X_1 X_4] + E[X_1 X_3] E[X_2 X_3]\big] \nonumber\\
    &-2 \big[ EX_2 X_4] E[X_2 X_3] + E[X_2 X_4] E[X_2 X_3]\big] \\
    &= (E[X_1^2] + E[X_2^2])(E[X_3^2] + E[X_4^2]) \nonumber\\
    &+2 \big[ E[X_1X_3]  [ E[X_2X_4] - E[X_1 X_4]] +  E[X_2 X_3] [ E[X_1 X_4]  - E[X_1 X_3] ]\big] \nonumber\\
    &-2 \big[ EX_2 X_4] E[X_2 X_3] + E[X_2 X_4] E[X_2 X_3]\big] \nonumber\\
\end{align}
But we have $Cov(A,B) = E[ (A-E[A])(B-E[B])] = E[AB] - E[A]E[B]$ and 
\begin{align}
Cov(A, B)Cov(C, D) &= E[ (A-E[A]) (B-E[B])] E[(C-E[C])(D-E[D])]\nonumber\\
&= E[AB]E[CD] =  E[ABCD] - Cov(AB, CD)
\end{align}

\newpage
\section{Notes on PCA1 of van Waaij et al}
van Waaij et al. suggest at PCA on a matrix of the following form for PCA
\url{https://doi.org/10.48550/arXiv.2302.04596} (their PCA1). This approach is further motivated by eq 7 in \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6707457/}, which is a very technical (but probably useful) reference.

\begin{equation}
    \hat{\mathbf{H}} = \frac{1}{m}\mathbf{X}^T\mathbf{X} - \hat{\mathbf{D}},
\end{equation}
where $\MX$ is the (uncentered) genotype matrix and $\hat{\mathbf{D}}$ is a matrix of diagonal entries ``correcting'' heterozygosities and $m$ is the number of SNPs.

In particular $d_{ij} = 0 $ for $i \neq j$ and, 
\begin{equation}
    d_{ii} = d_i = \frac{1}{m}\sum_{k=1}^m x_{ik}(2-x_{ik}),
\end{equation}
for diploid data. The correction term given by Reich 2009 and Patterson 2012 is different by a factor of four:
\begin{equation*}
    d_{ii}' = \frac{1}{m}\sum_{k=1}^m \frac{x_{ik}(2-x_{ik})}{4} = \frac{d_{ii}}{4}
\end{equation*}.
The difference is explained that they use allele frequencies instead of genotypes, i.e. their approach would use (again for diploid data)
\begin{equation*}
    \hat{\mathbf{H}'} = \frac{1}{m}\frac{\mathbf{X}^T}{2}\frac{\mathbf{X}}{2} - \frac{\hat{\mathbf{D'}}}{4} = \frac{\hat{\mathbf{H}}}{4},
\end{equation*}
and so the parametrizations are equivalent, but will differ by a factor of four.

Thus, (TODO: double-check if rows/columns are aligned, sum should be over SNP)

\begin{subequations}\begin{align}
    h_{ii} &= \frac{1}{m}\sum_{k=1}^m x_{ik}^2 - d_i \\
    h_{ij} &= \frac{1}{m}\sum_{k=1}^m x_{ik}x_{jk} 
\end{align}\end{subequations}
Consider now
\begin{align}
    f_{ij} &= h_{ii} + h_{jj} - 2 h_{ij}\nonumber\\
     &= \frac{1}{m}\sum_{k=1}^m x_{ik}^2 - d_i + \frac{1}{m}\sum_{k=1}^m x_{jk}^2 - d_j - \frac{2}{m}\sum_{k=1}^m x_{ik}x_{jk}\nonumber \\
    &= \frac{1}{m} \sum_{k=1}^m (x_{ik} - x_{jk})^2 - d_i - d_j \\
    &= F_2(i,j)\nonumber
\end{align}

Hence the matrix $\hat{\mathbf{H}}$ can be used to estimate $F_2$-statistics (possibly instead of PPCA?).
The detailed justification of this can be found in \href{https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional}{this statsexchange post  that will need to be adapted}.

Thus, this might be a useful alternative to PPCA to calculate $F$-statistics:

\begin{enumerate}
    \item Calculate $\hat{\mathbf{H}} = \frac{1}{m}\mathbf{X}^T\mathbf{X} - \hat{\mathbf{D}}$
    \item Double-Center $\hat{\mathbf{H}}$: $\mathbf{H}_c = \mathbf{C}\hat{\mathbf{H}}\mathbf{C}$, where $\mathbf{C}$ is a centering matrix
    \item Obtain PCs using an eigendecomposition of $\mathbf{H}_c$: $\MP\MP^T = \mathbf{H}_c$
    \item Calculate $F_2$ from the smaller space $\MP$
\end{enumerate}

\section{Standard errors and effective number of SNPs}
An issue in calculating standard errors for $F$-statistics is that SNP are usually correlated, and so standard variance and standard error calculations will fail.

let us assume $n$ populations can be represented by some population structure model that is parameterized by some covariance matrix $\MX$. We do not observe SNPs, but rather we have a noisy sample $\MG_{[S \times n]}$ at $S$ loci.

Let, as above, denote the data matrix as $\MG$, which is a noisy version of an allele frequency matrix $\MX$, and we assume we can do a PCA on some estimate of $\hat{\MX}$ as 
$\ML\MZ = \hat{\MX}$ where $\ML$ are the orthonormal SNP-loadings and $\MZ$ are the PCs. For example, we could do that using probabilistic PCA or the Cabreros-Storey-PCA

We are interested in statistics of the form 
\begin{equation}
    F_{ij} = (X_i - X_j)^2
\end{equation}
which can be estimated from $\MG$ using the unbiased estimator of Patterson et al.
\begin{equation}
    f_{ij} = \frac{1}{S}\sum_{s=1}^n (g_{si} - g_{sj})^2 - H_i - H_j.
\end{equation}

alternatively, we can also obtain an estimate from the decomposition of  $\hat{\MX}$ as
\begin{equation}
    p_{ij} = \frac{1}{S}\sum_{p=1}^n (z_{si} - z_{sj})^2,
\end{equation}
this sum is over PCs. This estimator has thus the advantage that it can be computed a lot faster since typically $n \ll S$, and if $X$ is low-rank we can even truncate the sum

An issue is the calculation of the standard error of $f_{ij}$ and $p_{ij}$. A simple estimator is 
\begin{equation}
    \sigma_p = \sqrt{\frac{\operatorname{Var}(p_{ij})}{S} }
\end{equation}
a problem with $\sigma_p$ is that SNPs are not independent, and so the variance estimates are underestimated. For this purpose, a block-jackknife can be used.

We block data using a vector $b$, s.t $b_i = j$ means that the $i$-th SNP is in block $j$. For each block, we then have the pseudovalue 
$$\Tilde{f}^{(j)} = \frac{1}{m_j} \sum_s I[b_s =j] (x_{si} - x_{sj})^2  $$
where $I[\cdot]$ is an indicator and $m_j$ is the number of entries in block $j$.

then
\begin{equation}
    \sigma_f' = \sqrt{ \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}( \Tilde{f}^{(j)} - f)^2  \right] }
\end{equation}

This motivates the effective number of SNPs, 
\begin{equation}
    S_e = S \left(\frac{ \sigma_f}{\sigma_f'} \right)^2
\end{equation}

which gives the number of pseudo-independent observations.

\paragraph{simplifying block-JK}
Writing
$$f = \frac{1}{n}\sum_s (x_{si} - x_{sj})^2$$


\begin{align}
    \sigma_f'^2 &=  \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}( \Tilde{f}^{(j)} - f)^2  \right] \\
    &= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \left[ \frac{1}{m_j} \sum_s I[b_s =j] (x_{si} - x_{sj})^2\right] - f\right)^2  \right]\\
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \left[ \frac{1}{m_j} \sum_s I[b_s =j] (x_{si} - x_{sj})^2\right] - \left[\frac{1}{n}\sum_s (x_{si} - x_{sj})^2\right] \right)^2  \right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \sum_s \left[ \frac{1}{m_j} I[b_s =j] (x_{si} - x_{sj})^2 - \frac{1}{n} (x_{si} - x_{sj})^2\right] \right)^2  \right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j} \left(  \sum_s \left[ \frac{  I[b_s =j] n (x_{si} - x_{sj})^2 - m_j(x_{si} - x_{sj})^2 }{n m_j}   \right] \right)^2  \right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}\frac{1}{n^2m_j^2} \left(   \sum_s \left[  (x_{si} - x_{sj})^2 ( I[b_s =j] n  - m_j)    \right] \right)^2\right]\\    
&= \frac{1}{g} \sum_j \left[ \frac{m_j}{n-m_j}\frac{1}{n^2m_j^2} \left(   \sum_k (z_{ki} - x_{kj})^2 \underbrace{ \left\{n \sum_{s \in b}  l_{sk}^2  - m_j \right\}}_{ v_{bk}} \right)^2\right]\\    
\end{align}
Due to the indicator, this does not simplify easily.




This procedure can be run for any statistic. In particular, we could also do that on some estimates of $X_1$, i.e. columns of the denoised data:

\begin{align}
    \sigma^2_{x} &= \frac{1}{n} \sum_s (x_{s} -\bar{x})^2  \\
    &= \frac{1}{n} \sum_s \left( \sum_k  l_{sk} [z_{k1} - \bar{z}] \right)^2\\
    &= \frac{1}{n} \sum_s \left( \sum_k  l_{sk}^2 [z_{k1} - \bar{z}]^2 + 2\sum_k\sum_{k'}  l_{sk}l_{sk'} [z_{k1} - \bar{z}][z_{k'1} - \bar{z}] \right)^2\\  
    &= \frac{1}{n} \left [ \sum_k [z_{k1} - \bar{z}]^2 \left( \sum_s  l_{sk}^2\right)\right]  + \frac{2}{n} \left[ \sum_k\sum_{k'}  [z_{k1} - \bar{z}][z_{k'1} - \bar{z}] \left( \sum_s  l_{sk}l_{sk'}\right)\right ]  \\
    &= \frac{1}{n} \left [ \sum_k [z_{k1} - \bar{z}]^2 \right]
\end{align}

where we use the PCA-decomposition
\begin{equation}
    x_{s1} = \sum_k l_{sk} z_{k1}
\end{equation}
and the fact that the $l$ are orthonormal.

\section{Squared standard error/variance}
For  normal RV $X,Y$, $\COV{X^2}{Y^2} = 2 \COV{X}{Y}^2$.

Let $X_i^2$ be a vector of squared allele frequencies for individual $i$. We can think of $X_i$ as the location of the individual in an $s$-dimensional coordinate system, or, equivalently, the distance from an arbitrary point. This could either be the reference genome (represented by the zero vector), or the data centroid (for mean-centered SNPs). In either case, $ \sum_s x_{is}^2$ is the squared distance from that reference point, and $\sigma_i = \frac{1}{S} \sqrt{\sum_s x_{is}^2}$ is an estimate of the standard deviation per dimension. 


Alternative: Following the probabilistic PCA model, there are three sources of variation in the data:
\begin{enumerate}
    \item Biological variation due to shared ancestry. These are the principal components $\MP$ we aim to estimate
    \item Sampling error due to a finite number of loci. These are given by the $\ML$. Each entry of $\ML$ is normally distributed, but rows are typically not independent. Furthermore, for genetic data they are typically not independent.
    \item Sampling error due to finite samples from each population. These are given by the heterozygosities
\end{enumerate}

\begin{align}
x_{s} &= \sum_k \ML_k z_{sk} + \mu + \epsilon_s\\
x_{s} - x_{t} &= \sum_k \ML_k (z_{sk} - z_{lk}) +  \epsilon_s -  \epsilon_t\\
(x_{s} - x_{t})^2 &=  \left(\sum_k \ML_k (z_{sk} - z_{lk} ) +  \epsilon_s - \epsilon_t\right)^2\\
&= \sum_k(z_s - z_l)^2 + \epsilon_s^2 + \epsilon_t^2
\end{align}
because the error terms are independent of the $z_s$, and $\ML$ is an orthonormal basis such that $\sum_k {L_k}^2=1$ and $\sum_{k, k'} {L_k L_k'} = 0$

This shows that the \emph{sampling errors} are simply independent of the $F$-stats, and can be obtained e.g. from the heterozygosities (following Chen \& Storey, as in van Waaij et al). They also correspond to the ``correction terms''.

This is just o



\section{Introduction-not used }
A crucial first goal in any population genetic analysis of a natural population is to describe the observed genetic variation in the system. A major topic of population genetics is to link these patterns of variation with the underlying processes, including genetic drift, migration and selection.

Admixture followed by drift can influence and shape the genetic diversity of populations. An important goal of many population genetic studies is to identify the admixture events that have left their mark as patterns of variation in human populations. 

Genetic variation in a natural population is shaped by many processes, including genetic drift, selection, and admixture. Admixture followed by drift is one of the most prevalent process that shaped human genetic diversity 



\bibliographystyle{plain}
\bibliography{references.bib}





\end{document}


